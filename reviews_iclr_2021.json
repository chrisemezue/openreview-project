[{"forum": "vT0NSQlTA", "title": "On the right track but more experiments are necessary", "reviews": {"review0": "The authors proposed latent optimistic value exploration (LOVE) as a mechanism to leverage optimistic exploration for continuous visual control. The main idea is to use a small (~5) ensemble of latent models with shared encoders (and therefore shared learned latent space) but different transition, reward and value models. The variance of predictions from this ensemble can be used as uncertainty estimates of each action sequence while the mean is the typical policy learning objective. Then LOVE puts more weighs on the states with high variance during exploration to enforce the agent to visit (optimistically) uncertain states. This is inherently optimistic because there is a positive bias (beta > 0 and var > 0) addition to the expected reward. \n\nThe idea of the paper is not novel and it has been explored before such as (T. Kurutach et al ICLR 2018) and unfortunately the authors did not provide enough context in the related works to distinguish their work with previous research. However, in its proposed form,  LOVE  is new to the best of my knowledge. Using the variance of predictions of an ensemble of agents is an interesting way of approximating uncertainty and the experiments demonstrate how this bias can improve the results. However, I find the experiments not to be convincing enough that the added complexity is necessary to achieve the improved performance:\n\nFirst, the main claim of the paper is that the proposed method achieves better score by exploring better. However, there are many changes that can cause this improvement. For example, using an ensemble of the models with different transition, reward and value models is essentially using a bigger (i.e. with more parameters) model. There is an ablation study that demonstrates only using an ensemble doesn't work (LVE alternate), however, this is not convincing that the improvements are coming from better *exploration*. This can be done in multiple ways such as demonstrating that a base method (e.g. Dreamer) works better if trained on the data collected by LOVE. An ablation study with a negative beta can be also super helpful (although I believe the results of that is kinda trivial).\n\nSecond, there is no study that visualizes the importance of the ensemble. The authors used 5 particles throughout the experiments but why 5? I'm not suggesting optimizing this number as a hyper-parameter or anything like that but what I'm looking for is a study that clearly demonstrates that having a more accurate approximation of uncertainty is important. This can be achieved by studying the effect of number of particles on the performance of the agent. Visualizing the actual variance of the predictions and demonstrating that the predictions actually vary is also important. This tightly related to various values for beta which also requires another ablation study. There are also many other way of enforcing optimism that the proposed method can be compared to.\n\nOverall, the authors are on the right track. The paper (except for related works) is very well written and easy to read. However, more experiments are required to clearly demonstrate why the method is working and how. ", "rating0": "5: Marginally below acceptance threshold", "confidence0": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review1": "Summary\n--------------\n\nThis paper proposes LOVE (Latent Optimistic Value Exploration), a model-based exploration algorithm for POMDP or pixel-based control systems. The method builds upon Dreamer (Hafner et al. 2019) for learning latent models, and the variance of value estimates (one transition/reward/value model per particle) estimates the epistemic uncertainty of the world, which can be used as a UCB-style exploration reward. LOVE can achieve a comparable or better performance on a simple pointmass environment with a trap and standard DeepMind continuous control suites.\n\n\nStrength:\n- This paper studies a high-significance problem of learning representation for visual-based control and deep exploration. Self-supervised learning of latent representation and model-based exploration is an important and timely research problem to study.\n- Overall, the paper is written well, with a clear motivation and a good organization of the method. Presentation in terms of pseudocode, plot, table, and hyperparameters look great.\n- The method is straightforward and the idea behind the algorithm is very reasonable.\n\nWeakness\n- The paper has limited novelty; the core idea of model-based planning and disagreement-based exploration are taken from existing works, though combining them and making them work in a challenging pixel-based continuous control tasks would be a non-trivial work.\n- Crucially, there are a few of *very* similar works in model-based exploration, which also builds upon Dreamer (please see the detailed comments below). There might be some technical difference, but a comprehensive comparison with existing works would be critical.\n- It is not clearly discussed or mentioned in the paper why the proposed method can be called as a \u201cdeep\u201d exploration, or how beneficial it would be compared to \u201cshallow\u201d explorations.\n\nDetailed comments:\n-----------------\n\nThere are some similar works in the model-based exploration literature, which this paper did not cite or compare with.\n\nPlan2Explore (Sekar et al., ICML 2020): this paper learns a Dreamer-like model, where an ensemble of $q(h | s, a)$ is learned and the variance of prediction (of the latent variable $h$) is used as the uncertainty, or the \u201cdisagreement\u201d intrinsic reward. The difference to LOVE would be, the uncertainty is measured by estimation of value function or latent representations. It will be interesting to see how these approaches can be compared.\n\nValue estimation might be much harder than the transition model, especially when the task reward is sparse, and would not be directly applicable in a reward-free/unsupervised setup. In such scenarios, how can LOVE learn to explore the world?\n\nMore related works to consider:\n- Ready Policy One (RP1; Ball et al., arXiv 2020)\n- Model-based Active Exploration (MAX; Shyam et al., ICML 2019)\n- Learning Awareness Models (Amos et al., ICLR 2018)\n\nIn terms of experiments/empirical evaluation, there is no baseline exploration algorithm presented that is based on Dreamer or pixel-based control, which makes the effectiveness of the proposed approach a bit difficult to be assessed. For example, what if we do a straightforward exploration (such as Curiosity [Pathak et al. 2017], RND [Bruda et al., 2018]) on Dreamer?\n\n\n**Additional Comments**\n\n- It would be great to provide more clarification/explanation on how LVE (LOVE with $\\beta=0$) and Dreamer are different.\n- Additional analysis and experiments would also have strengthened the paper, such as: how does the algorithm performance differ under different values of the planning horizon? How does the performance of the algorithm vary under different number $M$ of ensembles?\n- As in Hafner et al., 2019a, it would be great to clarify the difference between $p(\\cdot)$ and $q(\\cdot)$ in their meaning, i.e. $p$ for distributions that samples from real environments, and $q$ for approximations.\n", "rating1": "4: Ok but not good enough - rejection", "confidence1": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review2": "The proposed method seems like a simple and effective combination of existing ideas and approaches. Overall while I like the approach but am leaning towards a reject since I think there are aspects in the experiment section that can be clarified and improved. However I am open to changing my decision if my concerns are addressed during the rebuttal period.\n\nStrengths:\n- The ideas presented are simple extensions of existing methods and are quite easily digested.\n- The proposed approach does seem to learn faster on some of the tasks considered although some aspects of this need clarification.\n- The experimental details presented in the Appendix are quite thorough and did help me understand some aspects of the work in more detail.\n\nIssues and points worth clarifying:\n\n- Apart from a number of points of clarification that are listed later, my main concern is with the experimental section of the paper. This work compares performance of the proposed method on 8 of the DeepMind Control Suite domains and one toy domain with very little ablation. In contrast, the original DrQ and DREAMER work which are used as baseline here show results on 15 and 20 control suite tasks on top of some results on Atari. \n- The number of domains becomes somewhat more important in light of the fact that on 2 of the 8 domains considered DrQ outperforms the proposed approach. The explanation regarding these 2 tasks having dense rewards does make sense but more data would help substantiate the claim.\n- Another concern I have is with regard to the presentation of the results in Figure 3. Most of the curves are cut off before they have converged. This makes it hard to  sanity check against the existing results from the DREAMER and DrQ paper and it remains unclear to me if LOVE performs asymptotically as well as the baselines on some of the tasks. Perhaps this information could be included instead in Table 1 which as far as I can tell does not currently present any new information that is not present in Figure 3.\n- Finally the paper could also include more ablations - especially regarding the effect of the ensemble size and how important the step increase schedule is for the beta parameter of the UCB objective.\n\nFinally there are some points which I don\u2019t fully understand based on my reading of the text and for which clarification would greatly help.\n- The main text mentions that LOVE has exploration noise turned off since exploration happens through the UCB noise. Was this the same setting used for the LVE where beta is set to 0? My understanding is that LVE does have exploration noise but it would be good for the authors to confirm this.\n\n- The parameter in the UCB objective Beta is progressively increased from 0 in delta steps of 0.001. Where does this number come from? Was this a hyperparameter?\n\n- Learning multiple DREAMER models is an interesting idea but the obvious issue with this is that it could bloat wall clock learning time. How much slower is the proposed approach to vanilla DREAMER  in terms of wall clock time? While understandably the focus of this work is data efficiency, I think this is an important number to mention perhaps even in the Appendix to paint a more complete picture.\n\n--Update (Nov 25)--\n\nI am happy that the authors improved the paper with reviewer feedback. In particular I think the new ablations and comparison and mention of previous work makes the work more complete. The results on new sparse tasks are also interesting. I still think more can be done in terms of experimental validation (in particular my original note regarding early cutting of the curves has not been addressed). However overall I think the paper does meet the acceptance threshold as things stand.\n", "rating2": "6: Marginally above acceptance threshold", "confidence2": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review3": "---- Summary ----\n\nThe paper proposes LOVE, an adaptation of DOVE (Seyde\u201920) to latent variable predictive models (Seyde\u201920 only condsidered predictive models without latent variables). Seyde\u201920 proposes to use a generalization of Upper Confidence Bound to deep model-based RL, by training an ensemble of models and value functions, and training a policy to maximize mean + variance of this ensemble (similarly to Lowrey\u201919). The submission empirically demonstrates that tuning the learning rate and number of training steps per environment steps of Dreamer (Hafner\u201920) improves sample efficiency, using an ensemble of predictive models further improves data efficiency slightly (on cartpole and walker tasks), while on top of that the proposed exploration method slightly improves sample efficiency on the Hopper and sparse Cartpole tasks. \n\n---- Decision ----\n\nThe submission contains little technical novelty over prior work of Seyde (2020). The experimental results are weak, but somewhat justify the claims as there is a slight but consistent improvement on some tasks. However, the paper suffers from a major flaw in the empirical evaluation. Figure 3 and the relevant discussion describe LOVE as significantly outperforming the Dreamer baseline. This difference is largely due to the fact LOVE uses a different learning rate and number of epochs, which improves sample efficiency. The paper graciously provides the comparison to the fairly tuned baseline in the appendix as Figure 9, confirming this. The fairly tuned baseline needs to be moved from the appendix to the main paper, and the contribution section and the discussion of the experiments need to be rewritten accordingly. If this is provided, I will reevaluate the paper. In the current state of the paper, I am unable to consider its merits on the basis of this flaw.\n\n---- Strengths ----\n\nThe paper is technically correct (except for the flaw explained above), and proposes a promising approach to a relevant problem of exploration in RL from images. The experimental results indicate that the proposed method could be effective.\n\n---- Weaknesses ----\n\nThe major flaw of the paper is described earlier. In addition, there are two other major issues with experimental evaluation.\n\nThe experimental evaluation of the paper is rather weak. The proposed exploration method only improves performance in 2 out of 8 environments. This might be because the other environments do not require sophisticated exploration, in which case the method needs to be tested on more than 2 relevant environments. Sparse-reward versions of the evaluated environments can be easily designed and would be suitable for evaluating the method.\n\nThe second major issue is that the method is not evaluated against any competing exploration baselines, even though the paper cites multiple prior works on this. For instance, the paper claims that methods based on information gain or improving knowledge of the dynamics will not explore as efficiently as the proposed method. Both of these claims need to be empirically evaluated or toned down.\n\n---- Additional comments ----\n\nThe related work section is missing the following papers: \n- Ball\u201920 is a model-based RL method that uses UCB for exploration\n- Sekar\u201920 is a model-based RL method that uses latent ensemble variance for task-agnostic exploration\n\nBall\u201920, Ready Policy One: World Building Through Active Learning\n\nSekar\u201920, Planning to Explore via Self-Supervised World Models\n\n## Update \n\nThe new sparse tasks and comparison to Dreamer + Curious improve the paper and address some of my concerns. Specifically, a sizable improvement due to exploration is now seen on 3 tasks, Hopper, Cartpole Sparse, and Cheetah Sparse. The new maze task is also more challenging than the bug trap task.  \n\n--- Final Decision ---\n\nAfter the significant improvements in the experimental evaluation, I believe the paper provides a reasonable case for the proposed latent UCB method. It also provides an interesting discussion on the advantages of UCB-style methods, and an interesting observation that optimistic reward-based exploration can be effectively used even in absence of (positive) rewards. Even though the experimental evaluation of prior work on exploration is still rather lacking, I believe that these contributions are enough for the paper to be interesting to the ICLR audience. I raise my score to 6.\n\n--- Remaining weaknesses ---\n\nThe experimental evaluation in the paper is still quite lacking in terms of baselines, making it impossible to judge whether the paper actually works better than prior work.\n\nFirst, the proposed method contains two improvements, model ensembling, and optimistic exploration, but doesn't go much in-depth analyzing either of these improvements, instead trying to focus on both at the same time. This makes comparison to prior exploration methods hard because the proposed method receives an additional boost due to an ensemble of dynamics (the paper conveniently quantifies this boost in the LVE method, and it is shown to be rather large). For a more fair comparison, the ensemble of dynamics might be ablated (leaving only the ensemble of value functions), or the competing baselines could also be built on top of LVE.\n\nSecond, the paper only compares against one competing exploration method, Dreamer + Curious. There has been a large amount of proposed exploration methods, and it would be appropriate to evaluate the proposed UCB method against at least a few of them. For instance, the paper could compare against similar value function ensemble techniques (Osband'16, Lowrey'18, Seyde'20), or other cited work (Ostrovski'17, Pathak'17). Burda'18 is not cited, but perhaps should be compared against. All these methods can be relatively easily implemented on top of LVE for a fair comparison.\n\nBurda'18, Exploration by random network distillation.\n\n--- Additional comments ---\n\nWould be great to clarify what is the observation space for the bug trap and maze tasks. For instance, you could add observations and predictions for these tasks to the appendix.", "rating3": "6: Marginally above acceptance threshold", "confidence3": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}, {"forum": "_IM-AfFhna9", "title": "Interesting take on Unifying VCL and EWC", "reviews": {"review0": "The authors propose Generalized VCL in this paper, which consists of multiple ideas: first, the authors introduce a beta-Elbo, which facilitates downweighting the KL-term of VCL. If beta taken to the limit towards zero, the authors show that the beta-elbo recovers the online EWC learning criterion, which draws an interesting link between VCL and EWC.\nThe authors also discuss reweighting terms to introduce a parameter lambda as in EWC, which they incorporate via a lambda-kl divergence term.\nFinally, furnished with this learning objective that interpolates between VCL and EWC, the authors propose to combine the learning objective with the architectural choice of Film layers, which they show facilitate overcoming the pruning behavior that their method inherits from VCL by offering ways to prune nodes without injecting noise into the network.\n\nExperiments are broad on multiple interesting datasets and quite clearly show that their proposed combined model performs best.\n\nPositives:\nThe paper draws an interesting unification between EWC and VCL, and in fact also other related works, as subtle modifications in a regularizer. This by itself is an interesting contribution. The fact that the authors study the interplay of their learning arlgorithm with architectural biases, i.e. overcoming early pruning via film layers, is also a valuable idea that I find not just interesting in itself, but also stylistically valuable as an approach to studying  deep learning. While the Film layers per se also appear somewhat ad hoc, their empirical benefits -particuarly when paired with the lambda-elbo, are impressive and well put together.\n\nCriticisms:\nWhile I really enjoy the derivation of the beta-elbo in the zero limit, I found the introduction of the reweighting terms in Sec. 2.3 to be ad hoc and not particularly well justified. It feels as if it is reverse engineered to match the desired criterion from EWC. I think the authors should dig deeper here for better justifications for such choices, as they did a good job having a mathematically interesting framework to derive earlier.\n\nAdditionally, the film layers work great, but I maybe missed if they are the main attraction powering performance or if it is the combination with the new ELBO. Would film layers with VCL do equally well? This is empirically confusing, it would be great to get some more help to understand the relative merits of each components here and clarify more how these pieces fit together empirically. I do enjoy the appendix discussing this qualitatively, but I would like to understand it quantitatively better, as theoretically film layers plus VCL (without this paper's innovations) should also benefit similarly.\n\nOne additional criticism is that the title is somewhat misleading, as it does not generalize VCL to broader settings, but rather collapses it towards the limit beta towards zero. The title raised hopes for a richer variational treatment rather than a unification to EWC and an architecture change. The authors might want to consider tweaking the title to sth that is closer to the paper's actual contributions.\n\n\nOverall:\nThis paper takes an interesting approach towards adding to the EWC and VCL literature by unifying them and offering an architectural fix for a key problem in these scenarios. While the contributions are mixed and not consistently derived from clear modeling assumptions, their interplay is well studied and highly relevant to the understanding and improvement of practical continual learning. I also want to again applaud the authors for studying and explaining the interplay of pruning and film layers, I enjoyed reading the supplementary information on this. I wish more papers that discover methods that perform well empirically would study the interplays of algorithm and architecture similarly to expose interesting effects.", "rating0": "7: Good paper, accept", "confidence0": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review1": "This work considers online variational Bayesian approaches to continual learning. The authors propose a beta-ELBO objective which they claim interpolates between Gaussian variational inference (beta = 1) and Laplace\u2019s approximation (beta = 0).\nFurthermore, the authors propose task-specific, non-probabilistic (point estimation) FiLM layers that apply an element-wise transformation to the activations.\n\nTheory / Contribution:\nThe two contributions seem quite orthogonal to each other and each of them is rather minor in novelty. \nIt is obvious that using beta=0 leads to MAP estimates from which Laplace\u2019s approximation can be computed. However, I am quite confused what exactly the authors do here and there could be a major mistake:\nFrom the paper, I am not sure if the authors a) compute Laplace\u2019s approximation in the end, at the resulting mean of q, for any beta value? As far as I understand, the authors instead b) only optimise the variance through the beta-ELBO. \nHowever, in this case, the resulting approximation would *not* identical to Laplace\u2019s approximation!\nI need clarification what the authors are doing here.\nConsider the case of beta=0, the covariance will be the dirac distribution as the authors note in Sec. 2.2 or the supplementary material. The authors then go on and write the optimal covariance matrix for which the derivative of the beta-ELBO is zero.\nYou have first postulated that the covariance is zero, in order to be able to pull out the expectation, and then you again allow for a non-zero beta-elbo-minimizing covariance. This would be a contraction. This makes me guess you do compute Laplace\u2019s approximation instead. But then it is not discussed how you deal with beta>0. \n\n\nRelated work:\nThe related work section is rather short mentioning only very few related approaches. More effort is required here.\n\n\nExperiments:\nThe experimental evaluation is thorough and seems promising. Although I am wondering why e.g. Fig. 2 does not include VCL and EWC. Figure 8 in the supplementary material probably has some legends mixed up, or the explanations that small beta values cause locally measured locally are wrong? For Fig. a), the largest beta=10 seems to be a good approximation and also the most local. In case of Fig. B) and C) it is unclear / subjective (from visually inspecting the likelihood function) which is the best approximation. In A), beta=0.1 is the least local approximation, in B) beta=10 and in C) beta=1. I cannot follow the intuition provided here.\n\n\nSummary:\nI am sceptical about the correctness regarding the equivalence between VI and Laplace\u2019s approximation; the exact approach proposed in the paper is unclear and may be based on a contradiction. In case I have a misunderstanding here, I hope the authors will point this out and update the manuscript. \n\n\nUpdate after Rebuttal:\nThe authors provided clarifications and improved the manuscript. \nIn particular, the authors now detail the two special cases (beta=0, beta=1) and how it relates to EWC and VCL. \nI am no longer sceptical that the claims regarding the equivalence to EWC in case of beta=0 is correct. \nBased on this, I changed my evaluation and now suggest acceptance. ", "rating1": "7: Good paper, accept", "confidence1": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review2": "This paper proposes Generalized Variational Continual Learning (GVCL). It is shown that Online EWC and VCL are special cases of GVCL, along with other theoretical contributions. Further, GVCL is augmented with FiLM to alleviate weaknesses of VCL and GVCL. GVCL and GVCL-F are applied to a number of continual learning tasks and demonstrate competitive performance. \n\nAlthough GVCL and GVCL-F do not outperform baselines, particularly in hard settings (split-mnist and mixed vision), GVCL is an original and excellent contribution. The paper is clear and well-written, the proposed algorithm is theoretically motivated and analysed, experiments are comprehensive, demonstrating the empirical performance of GVCL. \n\nI have the following comments:\n- It would be interesting to have VCL and Online EWC added to Figures 2 and 3.\n- Why is GVFL significantly worse than baselines for split-mnist (Figure 2c)?\n- Why is split-mnist omitted from Figure 3?\n- The supplementary material contains some analysis on the effect and sensitivity of the value of $\\beta$ on the performance of the algorithm. This should be extended and presented in the main paper.\n\nMinor:\n- \"the node is *effective* shut off\" -> effectively", "rating2": "8: Top 50% of accepted papers, clear accept", "confidence2": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "review3": "This paper proposed a generalized variational continual learning (GVCL) framework using the \\beta - ELBO, and then combined with FiLM layers. The idea is interesting but there is a lack of preciseness. The pros and cons are as follows. \n\nPros: \n1. The proposed GVCL proposed a different and interesting perspective on the online EWC, viewed as a special case of \\beta \\to 0;\n2. FiLM layers are introduced to combine with GVCL, which lead to significant improvement in the performance;\n3. Various experiments are performed, showing some level of advantages. \n\nCons:\n1. The new perspective that online EWC could be viewed as a special case of the GVCL framework is lacking preciseness. First of all, as described in Sec. 2.3, the result of the \\beta-ELBO, even with \\beta \\to 0, does not lead to the key hyper parameter \\lambda in online EWC. To compensate this, the authors introduce a modified KL divergence to make them similar. However, it is not justified, from a unified Bayesian or some other theoretical perspective , why the previous \\beta-ELBO needs to be modified. It is kind of wired to start from the Bayesian  framework and then go back to the non-Bayesian perspective to design a Bayesian algorithm to improve the performance, and then claim that the previous non-Bayesian algorithm is a special case of the unified Bayesian framework. Moreover, as described in Sec. 2.3, the  resultant GVCL when \\beta \\to 0 is actually different from the previous online EWC algorithm. As a result, strictly speaking, it is not approperiate to claim that the online EWC could be recovered as a limiting case. \n\n2. If it is true that the proposed GVCL is a generalization of VCL and Online EWC, which allows interpolation between the two, then it is expected and reasonable that the GVCL alone (without additional FiLM layers) should perform at least the same as VCL and online EWC. Otherwise, the statement is not true and there is no advantage of the proposed GVCL framework . However, as shown in experimental results, e.g., Table 1, GVCL alone performs worse than Online EWC in large datasets, which is really wired. The authors also acknowledged this point and claimed that this is due to the difficulty in optimizing GVCL with small \\beta. It would be better to make such statement more precise because this is really important point for this paper. Otherwise, it implies that the so-called interpolation between VCL and online EWC has no additional advantage. \n\n3. Regarding the results of the GVCL and GVCL-F, it seems that the improvement mainly comes from the FiLM layers, rather than the GVCL framework itself.  To make this more clear and for a more fair comparison, it is highly suggested to compare other methods (online EWC, VCL, HAT, etc) with FiLM layers. Otherwise, the current improvement of the performance is unclear. In addition, the improvement of GVCL-F over the baseline is not consistent. \n\n\n\n", "rating3": "4: Ok but not good enough - rejection", "confidence3": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}, {"forum": "1Kxxduqpd3E", "title": "Interesting idea but weak experiment implementation and lack of motivation for the proposed method", "reviews": {"review0": "In the paper, Rotograd is proposed as a new gradient-based approach for training multi-task deep neural networks based on GradNorm. GradNorm is first formulated as a Stackelberg game, where the leader aims at normalizing the gradient of different tasks and the follower aims at optimizing the collective weighted loss objective. Under this formulation, one can utilize theoretical guarantees of the Stackelberg game by making the leader have a learning rate that decays to zero faster than the follower. To further account for the different gradient directions, a learnable rotation and translation are applied to the representation of each task, such that the transformed representation match that of the single-task learning. By adding an additional term accounting for learning this rotation, the leader in the Stackelberg game will minimize the loss to homogenize both the gradient magnitude and match the representation to single-task learning as close as possible. \n\nIn general, I find the direction of gradient homogenization for multi-task learning very important and interesting. The paper provides an interesting perspective through the Stackelberg game formulation, which provides a framework for selecting the learning rate of GradNorm type of gradient homogenization methods. The other contribution of the paper is a learnable task-specific rotation that aligns the task gradients with single-task learning. The proposing of a learnable rotation matrix seems an interesting idea, although I am not sure if it has been proposed previously for multi-task learning. \n\nI find the first contribution of formulating the problem as a Stackelberg game to be interesting and novel. However, in terms of the second contribution, I have some concerns about whether it makes the most sense by aligning the transformed representation with that of single-task learning. For MTL, one of the key benefits is learning a better representation by sharing it across different tasks to encourage helpful transfer between the tasks; by constraining the transformed representation to be as close to the single-task learning representation, it might limit the transfer between tasks since the representation are constrained to be equivalent to that learned by single-task learning. I think it is helpful to think about using rotation invariant representations for aligning the gradient directions, but it is questionable to align it to that of the single-task learning. \n\nAnother major concern is about the experimental results, full experiments are only conducted on one real-world dataset. The experiment on the second dataset seems to be very preliminary, which might not be sufficient to justify the proposed method empirically. Also on the second dataset, it seems the two different implementations of Rotograd have a large discrepancy in the results, which might need more investigation about why this happens. Meanwhile, many ablation studies seem to be missing. I am mostly interested to see experiments that validate the Stackelberg game formulation, for example by using different learning rates for the leader and the follower. Also, it would be interesting to see how the proposed Rotograd compares with pure GradNorm on gradient direction alignment. Overall, I feel the experiments are not complete for validating the effectiveness of the method. \n\nSome minor points: the description of d-grad method seems to be missing. Also, Yu et. al [2020] also deals with gradient aligning for MTL which could be considered as a baseline to compare with. \n\nYu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., & Finn, C. (2020). Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782. \n\n--------After author's response----------\n\nI am not fully convinced by the explanation of the motivation behind rotation matrix, in particular why it is aligning with the single-task learning, which is counter-intuitive. The authors provided more ablation studies, however, the evaluation on datasets is still quite preliminary with some questions remaining (such as why there is a discrepancy between the two versions of Rotograd on the second dataset). Therefore I am keeping my original score.\n", "rating0": "4: Ok but not good enough - rejection", "confidence0": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review1": "This paper presents an extension of Gradnorm to address task conflicting due to discordant gradient direction. Specially it introduces a rotation matrix to rotate the hidden representation from the last shared layer. The authors put the proposed method in the context of game theory to show stability and convergence of the training, which might be of merit. \n\nThe writing of the paper doesn\u2019t meet the publication standard, needing major work to improve. There are many typos and awkward sentences, hindering understanding of their work. Also, there are many places that need clarification, for example, in Proposition 4.1, the inverse of the gradient of Z with respective to \\theta needs to be calculated. So, what is the shape of this gradient matrix? How it is necessarily to be a square matrix? What ||\\Delta_{\\theta} Z|| represents? the F-norm? There is lack of adequate explanation of the motivation behind the objective in Eq. (6). By reading the paper, I have no idea about the two oracle functions, and why they are defined in the way shown in Eq. (8). \n\nEq. (3) is inaccurate, not aligning with that proposed in the GradNorm paper for the computation of L_{grad}^k.\n\nEq. (9) is problematic. Why R_k z_i^t does not appear in the objective function of the first optimization problem? If this is because z_i^{k,t} = R_k z_i^t + d_k, then the objective in the second optimization problem would be just 0. \n\nWhy operating on z instead of the gradient in Gradnorm can resolve the discordant gradient issue among tasks is not properly justified. \n\nThe reported empirical results are weak and do not support this method works as claimed.  \n", "rating1": "4: Ok but not good enough - rejection", "confidence1": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review2": "Summary:\n\nThis paper proposes an MTL method that encourages the gradients on shared parameters to have similar directions across different tasks. The motivation is to reduce conflicts between gradients of different tasks, so that training can proceed more smoothly, and fit multiple tasks more easily. The paper introduces a new way of thinking about this kind of method, i.e., through the lens of Stackelberg games, which could be useful in reasoning about the convergence of such methods. The method is shown to perform favorably against related methods, especially in regression settings.\n\n\n\nStrong points:\n\nMinimizing gradient conflict is a well-motivated way to reduce negative transfer.\n\nThe algorithm description is detailed, and should be straightforward for others to implement.\n\nStackelberg games are an interesting framework for thinking about methods like GradNorm and Rotograd that adaptively guide MTL training.\n\n\nWeak points:\n\nThe theory is interesting at a high-level, but it is not clear that it provides insights on what makes Rotograd work. In the paper, one main takeaway from the Stackelberg games framework is that the methods converge if the leader\u2019s learning rate is asymptotically smaller than the follower\u2019s. This takeaway is implemented by decaying the leader\u2019s learning rate, but it is not shown that this is a key point required for Rotograd to work. I would not be surprised if the results were unaffected if this decay were removed. If this point is really important, it should be illustrated in ablation studies. More broadly, since the point does not only apply to Rotograd, this ablation could also be done on Gradnorm and other methods. Such ablations would be one way to connect the theory to the methods.\n\nAnother main takeaway from the theory is that the rotation matrices and translation vectors should be updated with gradient descent, instead of simply replacing them each step. Intuitively, the algorithm would still make sense and be simpler if R and d were simply replaced. Experiments showing that the gradient-descent update rule is necessary would help show the value of the theory.\n\nSimilarly, the value of Proposition 4.1 is not clear. Is it to prove stability? Does this have some particular connection to Rotograd, or is it a useful fact about hard parameter-sharing methods in general?\n\nThere is one ablation \u201crotograd-sgd\u201d, but it is not clear how exactly it works: Can it simply update R and d however it wants, or is Eq. 9 still used to regularize the updates in some way?\n\nBy adding the rotation matrices, it\u2019s possible that information that would be useful to share across tasks is instead stored in these task-specific matrices. That is, conflict between tasks can beneficially lead to more general representations. Restricting R to be a rotation instead of any matrix is one step towards limiting the amount of information leakage into task-specific parameters. Is there a conceptual reason to expect that the benefits from reducing conflicts will outweigh this leakage?\n\nThe experiments are on an intentionally very small architecture, where one of the main issues is expressivity, which gives Rotograd an edge over methods that do not include an additional task-specific matrix. \n\nIn Section 5.1, does the method without Rotograd do poorly because there are no task-specific networks in that case?\n\nAlthough Rotograd is motivated to reduce negative transfer, Table 1 shows that Rotograd does not reduce negative transfer, but rather improves positive transfer. That is, uniform does better than rotograd in the tasks where single-task is better than multi-task, but rotograd does better than uniform in the tasks where uniform is already better than single-task. This makes me think that the benefits of Rotograd are not coming from reducing negative transfer, but from somewhere else.\n\nIs there an explanation for why Rotograd does not work as well for multi-class classification tasks (i.e., performs worse than all other methods for Left and Right)? Is it because the task-specific heads have larger output sizes? E.g., could it be better to have a separate rotation matrix for each class? Figure 4 in A.3 confirms that there is an issue here: the cosine similarity is not higher for rotograd for the classification tasks.\n\nOverall, from the limited scope of the experiments it is not clear that Rotograd would provide practical advantages over competing methods. The ChestXray experiments show that although Rotograd does not hurt much, it does not help overall compared too uniform.\n\nThat said, it would be still be interesting to see whether insights from Stackelberg games could lead to practical improvements for this problem.\n\n\nMinor comments:\n\nThe writing has some issues. These issues don\u2019t make the work unclear, but they are a bit distracting. Some example suggestions for fixing distracting word choice: \u201cpalliate\u201d -> \u201calleviate\u201d, \u201cspoiled\u201d -> \u201cnoted\u201d, \u201cwe have not being able to propose Rotograd, but also to derive\u201d -> \u201cwe have proposed Rotograd, and derived\u201d. There is also frequent non-standard mixing of em dashes with spaces and commas.\n\n\u201c$[r_k(t)]^\\alpha$ is a hyperparameter\u201d -> \u201c$\\alpha$ is a hyperparameter\u201d The hyperparameter is \\alpha, correct?\n\n----\n\nUpdate: \n\nI am very happy to see the new experiments that validate the implications of the Stackelberg games theory. The main drawback of the paper is that it is not clear that direction homogenization could lead to practical improvements for multi-task learning. The additional experiments in Table 2 are useful, and suggest that much of the benefit comes from the greater expressivity due to task-specific matrices.\n", "rating2": "4: Ok but not good enough - rejection", "confidence2": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}, {"forum": "5i4vRgoZauw", "title": "How much can we rely on BrainScore's metric in studies like this one?", "reviews": {"review0": "Summary\n-------\nThe paper is about ANN being best-known models of developed primate visual systems. However this fact does not yet mean that the way those systems are trained is also similar. This distinction and a step towards answering this question is the main motivation of this work. The authors demonstrate a set of ideas that while drastically reducing the number of updates maintain high Brain Predictability according to the BrainScore. The significance of this result in my opinion largely depends on how well we can map those observations and methods to biological meaning and knowledge on how primate brains are trained (see the discussion point below).\n\n\nCritique, Questions, Discussion\n-------------------------------\n(1) How good the \"match\" between the brain and DCNN is in the first place? For example, if we measure the match in terms of correlation (between responses, or predictions, any metric would work in the context of this question), then 80% of corr=1.0 would be very impressive and significant, while 80% of corr=0.2 (being corr=0.16) could well fall under the noise and while being significant numerically, does not give us the opportunity to say that we have captured 80% of the match between the artificial system and the ventral stream (because what we have actually captured is 80% of corr=0.2, which might as well be almost nothing).\n\n(2) \"squirrels to jump from tree to tree within months of birth\", \"macaques to exhibit adult-like visual representations after months\" -- hoe many synaptic updates happen during those months? Do we know? Maybe it is also in trillions? In which case this portion of the argument would fall apart. Emphasis on \"supervised\" would probably still survive.\n\n(3) \"a child would need to ask one question every second of her life to receive a comparable volume of labeled data\" -- are they not? I would say children get even more data if by \"question\" we will mean not only verbal questions and answers, but also answers that are tactile (\"how this will feel on touch?\"), auditory (\"what does this object sound like\"), visual prediction (\"will this thing now move to the right or to the left?\"), etc. Seen like this I would say that children receive tons of supervised data and \"one per second\" is an underestimation.\n\n(4) How does the \"match\" vary depending on random initialization? Is it consistently 54% or is there a substantial +/-?\n\n(5) How do we know the \"true zero\" in terms of the \"match\"? What would be a model (function? maybe a constant function?) that clearly has zero \"match\"? If we now take this function and run it through your pipeline to get the match%, would the result be indeed 0% or something else? Maybe 54% is the \"true zero\" and not 0%.\n\n(6) Why sampling from CORnet-S-based clusters of parameters is a good way of modeling \"at-birth\" situation? Compared to 54% achieved with this methods, what would be the match% if the network would initialized with vanilla Kaiming Normal? Uniform?\n\n\nRecommendation and justification\n--------------------------------\nMy main concern is with the interpretation of the meaning of this work. BrainScore's metric is a very approximate proxy that weakly reflects the match between models of vision. In this work, however, this metric is taken as a \"gold standard\" and it is assumed that achieving, for example 50% of BrainScore of 0.42 is something biologically meaningful. An ablation experiment that would demonstrate that achieving these 50% (or other numbers presented in the paper) is a non-trivial event which can only happen if the model is indeed becoming more \"brain-like\" would go a long way in making the case of this work strong. I suspect, however, that such an ablation study will show that there are ways to achieve high% of BrainScore using models that are completely dissimilar to the brain. I currently evaluate this submission as borderline, and am looking forward to authors' views on the concerns I have outlined above: do these indeed matter and affect the claims of this work (and how should we see them if that's the case), or are these concern largely irrelevant (and why we can ignore them if that's the case?).\n\n\nAdditional remarks\n------------------\nArguably missing references on modeling of the ventral stream with ANNs: https://www.nature.com/articles/s42003-018-0110-y, https://www.jneurosci.org/content/35/27/10005\n\n\nUPDATE - Nov 30\n-----------------------\nAfter looking at the revised version of the manuscript I am still concerned that the claims made in the abstract (and implied in the main text of the paper) about the match of ANNs to the brain are misleading the reader into assigning greater biological significance to the reported result than it actually holds. While the authors made slight modifications in the text and added a few sentences commenting on the issue, these changes did not constitute a change would make the reader \"extremely aware that when you say \"80% match\" you don't mean \"80% match to the brain\", but \"80% match to the score\"\". I find that a softer claim that would explicitly acknowledge that 5% of \"synaptic\" updates explain 80% of the predictivity score and not 80% of the match to the brain would make this work more scientifically precise and thus more valuable. I am keeping my original assessment of this paper as being borderline.", "rating0": "6: Marginally above acceptance threshold", "confidence0": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "review1": "The study starts from the fact that DNNs have been around and popular for a while for modeling the visual system, but that they are not realistic because they are trained via supervised learning approaches with a very large number of parameters and that this is not a feasible model of the development in the visual system.\n\nIn general, although the manuscript presents some interesting ideas, it makes many assumptions without providing clear bases for these assumptions (e.g. compressing the weights of a pretrained network to sample new weights is posed as a realistic approximation of the infant visual brain) and lacks a theoretical foundation for the claims and experiments that are presented. The authors acknowledge that this study is intended as a proof of principle, but given the arbitrary nature of the choices made, I do not see the added significant value of the results.\n\nWhile DNNs are indeed commonly used as models of the primate visual system, in my view, the current study is addressing a somewhat inconsequential problem. This is because to the best of my knowledge, no neuroscientist is claiming that a deep neural network is a complete and accurate model of the (development of the) primate visual system. Furthermore, it is well-known and acknowledged that deep neural networks are not biologically plausible models of (how learning occurs in) the brain. They are currently one of the best computational tools to use to study the sensory (and especially the visual) nervous systems, and that is all that they are. It is not clearly explained why it is necessary to claim that the learning in these models and the development of the brain has to be similar for them to be good models of vision. Of course, we should thrive for better and more accurate models of the brain, but in my view the current study does not serve to this goal.\n\nIn section 4 authors describe an initialization protocol for the network weights which involve compressing a trained model\u2019s weights into clusters and then sampling from these clusters. What is not clear is why the authors assume that this can be a valid model of the infant visual system. At this point their approach sounds like arbitrarily selecting a set of criteria to make the networks perform worse than fully trained networks, and then training them. I could be missing something, but I do not see the relevance or necessity of an approach such as the presented one. A main concern is that no theoretical basis has been established in the paper besides some superficial ideas. For instance, why would an infant brain be made up of a DNN with connections whose weights are initialized with the method authors came up with?\n\nMuch of the methodological details are only included in the appendix. I found it rather odd to not find any information about, for example, the proposed weight initialization method in the paper.\n\nIt is not clear to me what is presented in Figure 1 and why. Why are the authors showing how models from another paper trains?\n\nAnother concern is that nowhere in the results seems to be a test for significance. The improvements of the results could be a coincidence, since the results are heavily dependent on one experiment.", "rating1": "3: Clear rejection", "confidence1": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review2": "Summarize what the paper claims to contribute. \nPrevious work developed CORnet-S, a biologically inspired network that leads the Brain-Score benchmark of similarity with the primate ventral stream. A limitation of CORnet-S and other deep networks with high Brain-Scores is that they require many more weight updates than seem biologically feasible. In this paper, the number of weight updates used to train CORnet-S is reduced by two order of magnitude, while retaining a fairly high Brain-Score. This is done by combining three approaches, including reduced training, initialization of weights using compact distributions that describe trained weights, and updating only a minority of layers. \n\nList strong and weak points of the paper. \nStrong Points:\n-\tThe paper addresses an important problem that has not been given much attention previously\n-\tThe work builds on the state-of-the-art model in this domain\n-\tThe three approaches to reducing updates are complementary and interesting in different ways; the second and third thought-provoking with respect to their biological relevance\n-\tThe experiments and analysis are thorough\n-\tThe paper is well written\n-\tThe context of the work is clearly described and well referenced\n\nWeak Points: \nI wasn\u2019t able to discern any substantial weaknesses. \n\nClearly state your recommendation (accept or reject) with one or two key reasons for this choice. \nI recommend acceptance. The number of updates needed to learn realistic brain-like representations is a fair criticism of current models, and this paper demonstrates that this number can be greatly reduced, with moderate reduction in Brain-Score. I was surprised that it worked so well. \n\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. \n-\tIs the third method (updating only down-sampling layers) meant to be biologically relevant? If so, can anything more specific be said about this, other than that different cortical layers learn at different rates? \n-\tGiven that the brain does everything in parallel, why is the number of weight updates a better metric than the number of network updates? \n\nProvide additional feedback with the aim to improve the paper. \n-\tBottom of pg. 4: I think 37 bits / synapse (Zador, 2019) relates to specification of the target neuron rather than specification of the connection weight. So I\u2019m not sure its obvious how this relates to the weight compression scheme. The target neurons are already fully specified in CORnet-S. \n-\tPg. 5: \u201cThe training time reduction is less drastic than the parameter reduction because most gradients are still computed for early down-sampling layers (Discussion).\u201d This seems not to have been revisited in the Discussion (which is fine, just delete \u201cDiscussion\u201d).\n-\tFig. 3: Did you experiment with just training the middle Conv layers (as opposed to upsample or downsample layers)? \n-\tFig. 3: Why go to 0 trained parameters for downstream training, but minimum ~1M trained parameters for CT? \n-\tFig. 4: On the color bar, presumably one of the labels should say \u201cworse\u201d. \n-\tSection B.1: How many Gaussian components were used, or how many parameters total? Or if different for each layer, what was the maximum across all layers? \n-\tSection B.3: I wasn\u2019t clear on the numbers of parameters used in each approach. \n-\tD.1: How were CORnet-S clusters mapped to ResNet blocks? I thought different clusters were used in each layer. If not, maybe this could be highlighted in Section 4. \n\n", "rating2": "8: Top 50% of accepted papers, clear accept", "confidence2": "3: The reviewer is fairly confident that the evaluation is correct", "review3": "This paper presents an empirical study that elucidates potential mechanisms through which models of adult-like visual streams can \"develop\" from less specific/coarser model instantiations. In particular, the authors consider existing ventral stream models whose internal representations and behavior are most brain-like (amongst several other models) and probe how these fair in impoverished regimes of available labeled data and model plasticity (number of \"trainable\" synapses). They introduce a novel weight initialization mechanism, Weight Compression (WC), that allows their models to retain good performance even at the beginning of training, before any synaptic update. They also explore a particular methodology for fine-tuning, Critical Training (CT), that selectively updates parameters that seem to yield the most benefit. Finally, they explore these methods/algorithms' transfer performance from one ventral stream model (CORnet-S) to two additional models (ResNet-50 and MobileNet).\n\nPros:\nThe problem that the authors present is an interesting one and undoubtedly useful for many applications. Deep neural networks such as the CORnet-S, ResNet-50, and MobileNet are data-hungry, and obtaining labeled data is an expensive process (and perhaps even implausible in many cases). Techniques to condense these models in terms of parameters and alleviate the need for vast amounts of labeled data while maintaining desirable traits (such as brain-like representations) are important for the machine learning community. Though a bit far-fetched at this point, tracking the developmental trajectories of these neural networks can also have other scientific implications in the form of data-driven hypothesis testing.\n\nThe most exciting part of the study is the transfer experiment (from CORnet-S to ResNet and MobileNet). This seems like an interesting and novel way to construct model taxonomies. For instance, sampling from the CORnet-S weight clusters works well for ResNets potentially because these two models can be construed as \"recurrent\" in a way. MobileNets, on the other hand, are purely feedforward and thus are not significantly influenced by knowledge from the CORnet-S weights.\n\nMoreover, the authors conduct a series of numerical experiments to identify \"when\" their proposed methods are most useful. The finding that WC+CT is more advantageous in regimes where data is scarce (as opposed to regimes where data is plenty) is not surprising but a good one to report. I say \"not surprising\" because WT distills knowledge from a fully trained model, and CT only updates a fraction of the parameters (updating more parameters would require more data to prevent overfitting).\n\nCons:\nThe authors take the analogy between \"a developing visual system\" and \"training a model\" a bit too far. They operate under the premise that visual circuitry develops purely via \"supervised\" learning. Is there conclusive evidence for this? It is also surprising that discussions of reinforcement learning mechanisms never feature, given that these are more biologically plausible.\n\nThe novelty (and utility; for ex: Fig 2b) of the proposed initialization technique is marginal. It is not articulated how their method (WC) overcomes the critiques they raise against Frankle et al. 2019. Moreover, claiming that WC achieves decent performance with \"zero\" synaptic updates is not fair. This seems to be closer to restoring pre-trained weights than to random initialization (like KN). \n\nFor CT, the authors choose \"critical\" layers to update. Is there a rationale (or a statistical metric) that justifies choosing these specific layers? \n\nThe WC kernel cluster center visualization analysis (Fig. 5c) seems out of place and poorly discussed. What can be gleaned from the 3x3 kernels shown here?\n\nMinor:\nBy \"supervised updates,\" the authors refer to the number of available labels and not the number of parameter updates that happen. This terminology is non-canonical. \n\nEmploying Gabor priors for the first convolutional layer: Doesn't orientation selectivity emerge in the primary visual areas from experience, rather than structurally hard-coded? \n\nThe authors allude to the possibility of using \"local\" learning rules on a subset of layers identified by CT. However, this is speculation from the point of view of the current manuscript. All the conclusions drawn are from \"global\" gradients.\n\nAmbiguous sentence (Pg. 6, Sec 6): \"Reducing the number of supervised updates minimizes required updates by a smaller number of epochs and images.\"\n\n(Pg. 8) \"synaptic updates primarily take place in higher cortical regions\": Is there evidence for this?\n\nNumerical imprecisions:\n(i) The authors claim that the performance of CORnet-S_wc is 54% (relative to the fully trained model). However, in Fig 2b (mean) and Fig 3c (top) the markings seem to be closer to 50%?\n(ii) (Fig. 4a) The performance of MobileNet seems to be slightly better than CORnet-S, which contradicts the initial claim that CORnet-S is currently the best available model of adult primate visual processing.", "rating3": "6: Marginally above acceptance threshold", "confidence3": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}, {"forum": "BIIwfP55pp", "title": "review", "reviews": {"review0": "Summary:\n\nThis paper introduces PERIL, a meta RL method that combines demonstration trajectories and trajectories collected by the policy, in order to adapt to a new task. To this end, the authors combine ideas from metaRL (specifically from PEARL (Rakelly et al. 2019) and Humplik et al (2019)) where a set encoder is used to encode trajectories to a latent vector describing the task, with imitation learning techniques by (a) training this encoder also with demonstrations (b) initialising the latent vector at test time by feeding demonstrations through the encoder, and (c) having additional losses inspired by metaIL techniques. The motivation is that using demonstrations allows us to learn tasks that are difficult otherwise, for example because the rewards (at test time) are sparse. \n\nOverall impression:\n\nI like the idea of using demonstrations for metaRL when tasks are sparse. Many metaRL methods do not work well in sparse reward tasks, and using expert demonstrations is a nice way of guiding the agent towards behaviour that can solve the task. Empirically, the proposed method PERIL outperforms the baselines PEARL and MetaIL, so that is promising. The authors provide analysis of the latent space which nicely illustrates what the method has learned. However, PERIL is quite complex since it consists of many different parts and loss terms (six if I counted correctly), and it needs demonstrations + interactions + (sparse) reward signals at test time. I found it hard to keep track of everything and make sense of how these parts fit together. From both the text and the empirical results, it is not clear to me why all the parts are necessary / what they do, and I am left wondering if a simpler approach would work as well. The notation and mathematical formulation in the paper is not polished enough (there are inconsistencies, variable name clashes, some parts of the objective function not properly introduced and explained) which added to my confusion. Therefore, even though the idea seems promising, I think the paper is not quite ready for publication.\n\nQuestions:\n- In the introduction you say that MetaIL methods have the drawback that \"after adaptation, they cannot continue to improve the policy in the way that RL methods can\". You say that you method PERIL \"allows for continual policy improvement through further exploration of the task\". I have a few questions about this.\n - Since only the latent embedding is updated, doesn't PERIL also suffer from the fact that the policy cannot be improved in the way that RL methods can (but instead, all adaptation is that within the limits of task inference)? \n - Why is additional exploration at test time even necessary, if we have expert rollouts and the policy itself isn't actually updated (the only thing that's adapted at test time is the latent embedding)? If all the demos + trajectories are used for is task inference, then shouldn't the expert demos always be sufficient?\n - You motivate your approach by saying that at test time, it is useful if the expert does not have to provide a shaped reward. However, you do make use of a shaped reward during training - this is a limitation that should be discussed in the paper. In addition, you still need (sparse) rewards at test time. Are those really necessary, given that you have a demonstration of the task? Did you test PERIL without those sparse reward inputs to the encoder?\n- Table 1, how was the agent trained? Was it with number of adaptation trajectories k>0? If so, what if you would train the agent with k=0? On the other hand, can you get good zero-shot adaptation performance by just increasing the number of demonstrations? \n- You say $d_\\lambda$ is a VAE, but if I understand your setup correctly then $d_\\lambda$ is only the decoder of a VAE right? And Eq 8 is the reconstruction loss? Which also means it's not technically a VAE, because in encodes and decodes different things (encodes trajectories, decodes task descriptors - Humplik et al. 2019 describe this as an information bottleneck). Shouldn't there also be a KL term somewhere here?\n- Where is $L_{bc}$ used? It's not part of Eq (7), but I also can't find it anywhere else except in Algo 1 and Fig 2. And what about $L_{mi}$, it's only in Algo 1 but nowhere else? Fig 2 has $L_{KL}$, where is that from? It would really help my understanding of piecing everything together if there was one single equation somewhere, that includes all loss terms. For each loss term, I as the reader want to clearly understand where it comes from, and why it is necessary (see suggestions for additional baselines/ablations below).\n\nSuggestions / Feedback:\n- The problem formulation and the proposed solution don't match. In your problem setting you say you're in a general POMDP where the true state may be partially observed, but in your algorithm you rely on the fact that it's a POMDP only w.r.t. the task (i.e., reward and transition function) and *not* w.r.t. the environment state. That's an important difference! To explain that in more detail: in the introduction and problem setting you say $z$ models the true underlying state $s$ which can change at any moment: your transition function is $p(o',z'|o,a,z)$ where $o$ are observations. However, the entire formulation in your algorithm relies on the fact that $z$ does _not_ change over time, but instead describes a fixed task. That's also what PEARL does, which is what your formulation is based on. I think there's two ways to resolve this: (A) Either change the problem setting such that $z$ is fixed throughout time and define the transition function as $p(s'|s,a,z)$ where the environment state $s$ is now fully observable, or (B) change the algorithm to actually model a belief over a latent $z$ that can change over time. Option (A) is probably an easier fix, but then you might also have to change some of the environments (if I understand correctly, in Key2D the state of the handle is unobserved and can change the unobserved environment state).\n- Section 3.1, I would add explicitly what the objective of the policy is (both in writing and in a mathematical expression). You aim to maximise the return of a policy that conditions on $K_d$ demonstrations, and which has interacted with the environment for $K_r$ rollouts (changing the notation here to make the distinction clear). From there it is easy for the reader to see what happens if you set $K_d=0$ (you get something more similar to PEARL), and what happens when you set $K_r=0$ (which is the zero-shot case). It's good to contrast this for the reader, and explain / show empirically why and when $K_d>0$ and $K_r>0$ is necessary. (See my comment on baselines below.) \n- To understand PERIL better, I would suggest to add a few baselines. \n - PEARL with a pre-initialised buffer that contains the demonstrations. The encoder and policy will be trained as normal, but there's additional data coming from the buffer that contains expert trajectories. Since PEARL uses an off-policy algorithm it is possible to train the policy with this data. I think this is an important comparison, because it's a very simple way to incorporate demonstrations into PEARL and it would be good to understand if/when/why this works/doesn't work.\n - In addition to the above, use the demonstrations at test time to initialise the context in PEARL. This is very close to the setting in PERIL, except that some parts of the objective function are missing ($L_{info}$, $L_{aux}$, $L_{bc}$, $L_{mi}$ - I think).  This would give insight into whether those additional losses are truly necessary (currently you only have ablations on $L_{aux}$).\n - Zero-Shot PERIL. There is some analysis of this in Table 1, but I think it would still be helpful to add this baseline. Does it work well for within-task-distribution adaptation (Fig 4) and not so well for settings that require more generalisation (Fig 5)? What if we just throw in more demonstrations, is that sufficient to do zero-shot adaptation or do we really need the policy rollouts? I think this is a central question that should be very clearly answered in the paper. Table 1 is a good start but this analysis can be expanded.\n - Humplik et al. (2019), but with additional demonstration data to train the encoder/decoder. Again, this is the simplest way to incorporate the demonstration data into this method without explicitly making use of it at test time. This comparison would tell us something about why the demonstrations are necessary - are they necessary during training but not at test time, or the other way around, or are they necessary both during training and testing?\n - Not sure I got everything, there's still $L_{bc}$ and $L_{mi}$ which I'm not entirely sure where they come from and if they are necessary. But basically, I think it's really important to analyse which parts are necessary - and make the method as simple as possible if you find some parts are not necessary.\n\nSmaller comments (didn't influence my score):\n- There's a clash between using the variable k/K for the demonstrations (e.g., Sec 3.1 \"primal inference\", Sec 4.1 first sentence, Fig 7), and for the number of policy rollouts (e.g., algorithm 1 line 5, Table 1). This is confusing, so I strongly suggest using two distinct variable names (something like $k_d$ and $k_r$ also works).\n- Similarly it would help if you use two separate notations for the trajectories $\\tau$ that come from the demonstrations, and the ones that come from the policy. Throughout Section 3 I don't always understand which one of the two you are talking about.\n- Your references need fixing. Some of them are without a year, and some say technical report even though they were published at a conference (e.g. Finn et al., Rakelly et al.). Your \"Wang\" reference for RL2 also seems wrong (first sentence in related work)? It should be Jane Wang et al. \"Learning to reinforcement learn\". The way I always get my bibtex entries is via scholar.google.com: search for the paper there; click on the \"cite\" button under it and then \"BibTeX\" (Double check whether the paper was published somewhere though, google scholar often only puts the arXiv link then actually the paper was published somewhere. Sometimes the authors also put the correct bibtex comment on their homepage/github with the code).\n- Fig 1 left, there's a typo: \"learn to lean\" -> \"learn to learn\"\n- For your experiments, I would call PERIL-A only \"PERIL\" (since this is your full method, including all losses), and then call the *ablations* different, so for example PERIL-noAux when you remove the auxiliary loss.\n- All figures should have some form of indication of the error/std/confidence interval (using shaded regions around the mean for example).\n- Sec 4.2, explain what the multi-task family setting is and why it is challenging. \n", "rating0": "4: Ok but not good enough - rejection", "confidence0": "3: The reviewer is fairly confident that the evaluation is correct", "review1": "## Summary of work\nThis work proposes PERIL, a method for combined Meta Imitation Learning and Meta Reinforcement Learning using context-based meta-learning. Given a set of demonstrations, a latent variable representing the desired task is inferred, and trajectories are generated conditioned on the inferred latent variable.  The data from the expert demonstrations and trajectories are used for meta-learning updates.\n\n## Review\nKey comments below are included in bold text.\n\n* Related Work\n  * I don't agree that meta-learning was conceptualized as an RNN-based task. There are many variants of meta-learning, RNN-based, MAML (optimization based), Encoder-based (Neural Processes), etc.\n  * It seems that \"Scalable Meta-IRL Through Context-Conditional Policies\" (Ghasemipour et al. 2019) and maybe \"Robust Imitation of Diverse Behaviors\" (Wang et al., 2017), are closely related context conditional meta learning methods (similar to Yu et al. which you cited). They may merit citation.\n  * Bottom of page 2: Why do they have the traditional caveats? They are using rewards, so they should be able to do better.\n* Section 3\n  * Section 3.1: A number of works which also combine Meta-IL & Meta-RL use the setup you use as well (Primal Inference + Exploratory Adaptation). I think you should acknowledge this and provide citations (such as Mendonca et al., Zhou et al., Gupta et al. which you cited in other sections).\n  * Section 3.2: What is a Variational Encoder? This is not a standard term.\n  * Section 3.2: Please clarify how the KL relates to the mutual information. While this may be a simple connection, I think it should be explained better.\n  * __Section 3.2: Where is equation 1 coming from? It looks like the Evidence Lower Bound but $\\mathcal{G}(\\mathcal{T}|z)$ is not specified and could be a non-likelihood objective function. Please clarify.__\n  * __Section 3.2: You mention that you are using an encoder in the same manner as Rakelly et al., but you do not actually describe the method in your manuscript and how you perform context aggregation. Please include a complete description in the main manuscript.__\n  * __Section 3.2: First paragraph of page 4 requires significantly better clarification. Specifically the sentences \"This is generally not ... in unseen environments.\" It is not clear why you are saying your context-based approach is better.__\n* __Section 3.3:__\n  * __While you do acknowledge Yu et al. in the beginning of this section, you understate the extent to which this is similar to their mutual information objective. Equations 3&4, \"Learning from demonstrations\", and \"Linking variational posterior\" are effectively identical to Yu et al.'s with different variable names. You should clarify and attribute credit to their work in a more clear manner.__\n  * __I do acknowledge that you have tried to address the intractability of $\\mathcal{L}\\_{info}$ in a different manner than Yu et al., but the main manuscript would benefit from a brief explanation for why you took this route (even though you have derivations in the appendix)__\n  * __You need to explain why $\\mathcal{L}\\_{BC}$ is not included in your objective. From looking around in your appendix, I think this is because you are estimating the $p(z)$ marginal with the expectation distribution as in equation 5, so you don't take the gradient with respect to $q\\_\\phi$. This still needs explanation in my opinion.__\n  * __Section 3.4: First sentence, what do you mean by unsupervised? You are using the critic to train the model the generates $z$. Please elaborate.__\n  * __Section 3.4: Sentence before equation 9, doesn't this depend on what information is included in the vector $b$?__\n  * __General Question: What is the reinforcement learning algorithm you are using?__\n  * __Section 3.5: I don't understand what you mean by $\\mathcal{D}^{\\mathcal{T}}$?__\n* Section 4\n  * Please explain what the auxiliary informations are for each task.\n  * __Baselines__\n    * __As far as I can tell, you have not compared to any prior method that combines Meta-IL and Meta-RL (such as Mendonca et al. or Zhou et al. which you cite in your work). I think this is a big flaw of your experimental section. Including this would demonstrate whether your latent-based meta-learning approach is better or not from prior works with used other types of meta-learning, which should be your main value proposition. I think latent-based might work better than the MAML-based as in some prior work, but you have not provided any experiments to support this.__\n    * __Why are you comparing to noisy BC and not normal BC (also from your description of noisy BC is not clear what the method is)? Also please clarify what BC loss you are using (e.g. mean-squared error loss, maximum likelihood with learned variances, or whatever you used).__\n    * __I think it would be valuable to also include one setting where only the auxiliary loss + critic loss (no mutual info) is used to see the the value of the additional mutual information losses.__\n  * Section 4.1: How many train/test tasks in each task family. \n  * __Section 4.1: You mention sparse rewards here, but in the appendix there is information about \"dense rewards during meta-training\" for the critic. I don't quite understand what is happening here. Are you using sparse or dense rewards? And is every baseline method that uses rewards use the same type of reward? Please clarify.__\n  * __Section 4.2: Do you mean that you are training a single agent on all tasks simultaneously? How are you doing this when in the observation/action spaces are different? Do all these have the same observation/action space?__\n  * __Section 4.2: Last sentence of paragraph 1 needs elaboration. I don't think you have provided any explanation for these experiments, for example how you are setting up experiments for adapting to unseen dynamics.__\n  * __Section 4.2: Table 1, how is 0 possible? Also, you haven't provided explanation for how these values are computed.__\n", "rating1": "4: Ok but not good enough - rejection", "confidence1": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review2": "## Summary of the Work\nThe work propose a method which allows us to synthesize meta-RL and meta-IL, by pre-training and conditioning a context-based off-policy meta-RL algorithm on imitation data. Strongly inspired by PEARL (Rakelly, et al 2019) and meta-IL (Yu, et al 2020), this method outperforms previous methods by varying margins on a range of newly-introduced 2D and 3D robotics tasks. The work introduces several new design elements and losses to this family of methods, and the experiments do not make clear which ones are responsible for the increased performance. Additionally, it's not clear the included experiments can fully substantiate the long list of claims provided by the authors, not the least of which is that their method performs zero-shot adaptation to new tasks.\n\n## Pros and Cons\n\n### Pros\n* Addresses several important problems in adaptive RL\n - generalizing to complex tasks and outside-of-distribution tasks\n - using demonstration data to avoid costly random exploration\n - fine-tuning policies acquired with meta-RL/meta-IL\n* Hyperparameters and reward functions are well-documented\n* The visualizations are clear and helpful\n* Overall the work is well-organized\n\n### Cons\n* Makes many claims about the method which are difficult to fully substantiate in such a short paper\n* Treatment of prior work is limited to only a few methods from the past few years, and does not acknowledge many prior works in context-based meta-RL/MTRL. Relationship to the very-similar prior works PEARL and meta-IL is unclear.\n* Experiments section makes it very difficult to compare presented results to prior work\n* Lack of ablations make it unclear which (of many) new design decisions are responsible for the method's performance\n* Experiments don't appear to make a credible simulation of demonstration data which might be encountered in the real world\n* Claims of applicability to robotics necessitates comparisons with robotics benchmarks (See MetaWorld and/or RLBench).\n* Presented method is very complex\n* Experiments lack multiple seeds and any statistical significance tests\n* Terminology and notation often veers far from previous works and conventions, making it difficult to parse. Sometimes writing is ambiguous or unclear.\n\n## Evaluation\n### Quality\n3/5\nThe presented work and experiments are high-quality in their motivation, implementation, and usually their presentation. I think the quality of this work suffers when we consider how well it positions itself with respect to prior work (not well), and the level of detail with which it explores and substantiates each of its claims (of which there are many, making it impossible to address any of them fully). The method section is extensive and recapitulates in detail many concepts from RL, variational inference, IL, etc. It can probably be more sparse to make room for a better treatment of prior work and more experiments/analysis of the work's claims.\n\n### Clarity\n2/5\nThis work suffers from significant clarity issues, mostly around use of language (e.g. zero-shot learning vs few-shot learning), non-standard notation and terminology (e.g. \"primal inference,\" \"privileged information,\" etc.) and the use of equations which are not really necessary to demonstrate a point, \n\n### Originality\n3/5\nWhile original in nature (for instance, introducing privileged information, considering fine-tuning and off-distribution tasks, using demonstrations for pre-training, etc.), the work does not make it easy for the reader to divine how its contributions build on significantly-similar previous works which are highly-cited. This does it and the reader a disservice. \n\n### Significance\n2/5\nIf all claims in the work are well-substantiated, it can be a very significant work, but I don't believe they are substantiated. For instance, the introduction mentions low-dimensional observation spaces as a limitation of current meta-RL/IL work, but the experiments don't seem to contain any use of high-dimensional (e.g. image) observation spaces. It's not clear how the claim of zero-shot learning is supported.\n\n### Misc Editorial Comments and Reviewer's Notes\n\n#### Claims\n* Addresses three limitations of current meta-RL/IL\n - shaped reward functions\n - constrained low-dimensional action/observation spaces\n - requires hand-defining low-dimensional observation/action spaces\n* A hybrid framework which combines the merits of RL and IL\n  - tasks defined only using demonstrations\n  - unlike other (meta)-IL algorithms, allows for improvement of the policy after adaptation\n* Uses only proprioceptive actions from the agent, and implicitly recovers the external environment state\n* Allows for learning new tasks without requiring any expert knowledge in the human teacher\n* Achieves \"exceptional\" adaptation rates and is capable of exploiting demonstrations for efficient exploration\n* Outperforms other meta-RL and meta-IL baselines\n* Is capable of zero-shot learning\n* Is capable of multi-family meta-learning and out-of-family meta-learning through clustering in the latent space\n* Shows how we can use privileged information (during training) to create an auxiliary loss for training the embedding function, allowing us to recover the \"true underlying state\"\n\n#### Mechanisms\n* Represents a task by a latent vector, which is the belief state of the task given a demonstration\n* Meta-training encourages high mutual information between the demonstration data and latent space\n* After adaptation, the agent can explore and update the latent space\n\n#### 1. Introduction\n* \"hand-crafted, shaped reward functions...\" nothing in the meta-RL formulation requires shaped reward functions, as opposed to a sparse ones which are easier to craft. Granted, on-policy meta-RL algorithms are challenges by sparse reward settings in a similar fashion that on-policy RL algorithms are challenged by sparse reward settings, but this is a property of RL in general and not just meta-RL. [2] is a meta-RL method which can cope with sparse rewards, and is extensively-cited in this work.\n\n* \"defining a low-dimensional..\" this is not a limitation per se of meta-RL or meta-IL -- there's nothing in their formulation which necessitates meta-RL operating on low-dimensional state as opposed to images, though it is certainly a design challenge. See [1]\n\n* \"different task families\" Perhaps I have misunderstood the authors, but this seem orthogonal to the purpose of the work and of meta-RL. It is not immediately clear what the authors mean by \"task families.\" While there is certainly work on cross-domain transfer in IL and RL, adapting policies to different action and/or observation spaces is not a typical goal of meta-IL and meta-RL algorithms, so it seems strange to level this critique. \n\n[see \"claims\" above]\n\n#### 2. Related Work\n* Meta-learning and meta-RL far predates Wang and Duan. Please see [3,4, etc.]\n* Modern work on context-based meta-RL and adaptive RL predates PEARL. Please see [5, 6, 7] which all perform variational inference on trajectories to generate a latent context, which can then be used for adaptation\n\n#### 3. Method\n* The proposed approach seems hardly different than PEARL[2] with the following changes. The reviewer may have missed something, but given close relationship between these methods, please make crystal clear for readers the differences between this method and the substantially-similar PEARL method.\n  - Pre-training uses demonstration data rather than RL episodes\n  - This work studies what happens if you continue training after adaptation\n  - Introduces an auxiliary loss which allows conditioning on privileged information\n* 3.2: \"Traditional meta-RL methods leverage RNN-based\" - This is hardly true in any universal sense. Previous meta-RL method have used RNNs [8], variational inference [2], autoregressive models (attention)[9], hierarchy [10], exploration policies [11], etc. to implicitly model the latent task space.\n* 3.3: The included equations don't seem to add much to the paper's story and seem to recapitulate well-known results from RL, variational inference literature, or cited work.\n * 3.5: The use of a SAC expert trained on full-state versions of the environment is not a faithful simulation of expert data, which will be significantly noisier and lower-entropy than a SAC expert, and also is unlikely to be optimal according to any RL loss. I think that this reduces the IL aspects work to a form of offline RL where the offline data source is a SAC policy, and the results demonstrate that the method can reconstruct the privileged information which was available to the SAC agent. The authors note that they augment the demonstration data with \"imperfect demonstrations\", but are silent about how this is achieved (and it must be done with great care).\n\n#### 4. Experiments\n* 4.1: Though it is ambiguous from the text, these experiments seem to either present 1 experiment per method, or the average of 3 experiments per method. This is unfortunately not enough data to make a statistically meaningful comparison of the performance, especially considering the small performance differences involved. Please see [12] for a handy guide on how to compare performance. In short, you will likely need 10 seeds for each experiment and should conduct a statistical test to ensure your differences are real. Please include a 95% confidence interval in your plots for the benefit of readers.\n* 4.1: These experiments are meaningful and helpful, but it's also important to readers that they can verify you have reasonable implementations of the comparison methods. This necessitates providing some results for some of the environments used in Yu, et al and/or Rakelly, et al. How is the reader to know your implementation or hyperparameters are fairly representing the comparison methods?\n* 4.2: I think the reader would benefit from seeing t-SNE plots from the comparison methods as well. The presented plots look very similar to t-SNE plots generated by plotting samples from a PEARL posterior.\n* 4.2: It is very unclear what the authors mean by \"zero-shot\" learning. By my estimation, this method always requires some samples of the target environment to attain the presented performance, making it squarely a few-shot domain.\n* This work introduces many new design elements on top of PEARL and Yu et al, and it's unclear which of them are responsible for the observed performance. Please include ablations which compare your method's performance without each new design element, to demonstrate the impact of each.\n\n[1] https://arxiv.org/pdf/2006.07262.pdf\n[2] http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf\n[3] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.1796\n[4] https://www.sciencedirect.com/science/article/abs/pii/S0893608002002289\n[5] https://openreview.net/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf\n[6] https://arxiv.org/pdf/1809.10253\n[7] https://arxiv.org/abs/1806.02813\n[8] https://arxiv.org/abs/1611.02779\n[9] https://arxiv.org/abs/1707.03141\n[10] https://arxiv.org/abs/1710.09767\n[11] https://arxiv.org/abs/1802.07245\n[12] https://arxiv.org/abs/1904.06979", "rating2": "3: Clear rejection", "confidence2": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "review3": "Summary: This work seeks to efficiently learn new tasks by combining meta-RL and imitation learning (IL). Such a combination is a natural thing to try, as both lines of work improve sample complexity of learning a new task: meta-RL by leveraging experience on prior related tasks, and IL by leveraging demonstrations. Demonstrations also form a natural way of specifying a new task to the agent.\n\nThis paper extends an existing meta-RL approach (PEARL) to additionally leverage demonstrations. This leads to strong results on a set of 2D problems, as well as a 3D reaching task.\n\nSpecifically, PEARL is a Thompson-Sampling approach, consisting of two main components: (i) a learned posterior distribution $q(z \\mid c)$, which encodes a distribution over latent $z$\u2019s reflecting the task conditioned on previously observed states, rewards, and actions $c = s_0, a_0, r_0, \u2026$; and (ii) a context-conditioned policy $\\pi(a \\mid s, z)$, which is used for both exploration to infer the task, by producing more trajectories for the context $c$, and for solving the task, once uncertainty over $z$ is low. Specifically, this paper modifies PEARL in the following ways:\n- The context-conditioned policy is trained with an objective similar to behavior cloning to produce trajectories that match the demonstrations, in addition to learning from normal reward signal.\n- The posterior $q(z \\mid c)$ is trained to produce $z$\u2019s that encode information about the task, given demonstrations for $c$.\n\nStrengths:\n- This paper studies an important problem: how can we quickly learn new tasks? For many real-world RL tasks, we want policies that can quickly adapt to new tasks without retraining from scratch. This paper observes that prior approaches have drawbacks: IL on its own can be data-hungry, requiring additional roll-outs or many demonstrations; and meta-RL can be challenging with sparse rewards. Therefore, combining the two is a natural and promising direction to investigate.\n- The experimental results are generally quite encouraging. PERIL substantially outperforms the baselines, and can even generalize to a fairly wide distribution of 2D tasks (i.e., the same policy can learn to simultaneously do reaching, peg-placing, and key-rotating tasks, while existing works typically learn a narrower task distribution).\n\nWeaknesses:\n- Fairly strong assumption. This paper assumes that an expert distribution $p_{\\pi_E}(\\tau \\mid z)$ over trajectories conditioned on the learned latent $z$ is available. This seems to be a fairly restrictive assumption, since $z$ is learned by PERIL, and therefore, it seems unreasonable for an expert policy to also be able to condition on $z$. Instead, it would be nice if we could relax this to only condition on e.g., the observation $o$.\n- Clarity. While the high-level approach is clear, many of the details are confusing and unclear, which makes it challenging to evaluate this approach. I list the main points of confusion below.\n    - The problem statement defines the task in terms of $z$, which is confusing, because $z$ should be part of the approach, rather than part of the setting. In particular, it\u2019s unclear what it means for the dynamics model to condition on $z$. It seems like this may be mixing the learned latent $z$ with the true state $s$? More generally, the problem statement (Section 3.1) mixes the approach with the problem setting, which makes it confusing to understand what is a constraint due to the setting, and what\u2019s a design choice for the approach.\n    - The principled way to optimize Eq (3) is to maximize variational lower bound (Barber & Agakov, 2003), by substituting the posterior $p(z \\mid \\tau)$ with an arbitrary function $q(z \\mid \\tau)$. This appears to be what the paper is doing, but the current phrasing is pretty unclear. In particular, it\u2019s unclear to me how $\\mathcal{L}_\\text{info}(z)$ is optimized / defined. How is $p(z \\mid \\tau)$ defined? It\u2019s clear how you can do this in the case where the task descriptor is available, e.g., in Section 3.4, but in general, it\u2019s unclear what the learned $z$ should be. Is this from leveraging the latent space of the expert SAC agent? What is $\\pi_b$ in Equation 6?\n    - The notation for the task-dependent objective $G(\\tau)$ seems unnecessary and serves to distract \u2014 in particular, it\u2019s not initially clear why we need this and not just maximizing the expected discounted rewards. I would suggest removing this notation, and just saying at the end of the approach: \u201coverall, we minimize the following loss: $\\mathcal{L}_\\text{critic}(z) + \\mathcal{L}_\\text{info}(z) + \\mathcal{L}_\\text{aux}(z) + \\ldots$.\n    - There are quite a few undefined loss functions in line 12 of Algorithm 1, in particular $\\mathcal{L}_\\text{mi}$ and $\\mathcal{L}_\\text{D_KL}$.\n- Related work. This paper generally seems to lack appropriate citations in several key places.\n    - In the introduction, several key areas seem to require citations (e.g., citations for meta-IL, posterior sampling with meta-RL should cite PEARL, claims that meta-RL requires shaped rewards / claims that meta-IL cannot adapt afterwards).\n    - The following references seem highly relevant to related works section on exploration in meta-RL: [1], [2], [3].\n- Experiments.\n    - Why is the behavior cloning baseline with noisy demonstrations? It seems like the fair comparison should be BC w/o noise.\n    - This paper claims that PERIL is capable of exploring beyond demonstrations, but the tasks that this paper evaluates on don\u2019t seem to require much sophisticated exploration. Substantiating these claims seems to require evaluation on tasks requiring more exploration.\n\nI am initially recommending rejection, due to the aforementioned weaknesses. I believe that the related work and clarity could be improved during the rebuttal period, which would help me raise my score, although I find the strong assumption to be a fairly serious weakness.\n\nAdditional comments:\n- Ill-formatted citations. Many of the citations are missing the year, e.g., Zhou et al., Ross et al., Mendonca et al., Duan et al., Yu et al.\n- Contextons \u2014> contexts?\n- Variational Autoencoders are inconsistently abbreviated as VE and VAE. Seems like it should follow the standard of using VAE.\n\nReferences:\n[1] VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning. Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, Shimon Whiteson. Oct. 2019. ICLR 2020. https://arxiv.org/abs/1910.08348.\n\n[2] Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning. Evan Zheran Liu, Aditi Raghunathan, Percy Liang, Chelsea Finn. June 2020. ICML LifelongML Workshop 2020. https://openreview.net/forum?id=La1QuucFt8-.\n\n[3] Environment Probing Interaction Policies. Wenxuan Zhou, Lerrel Pinto, Abhinav Gupta. July 2019. ICLR 2019. https://arxiv.org/abs/1907.11740.", "rating3": "4: Ok but not good enough - rejection", "confidence3": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}, {"forum": "cO1IH43yUF", "title": "Useful tips of BERT fine-tuning for practitioners and insightful analysis", "reviews": {"review0": "This paper proposes a few tricks to improve the stability of BERT fine-turning, which include a standard Adam optimizer (with bias correction), the top BERT layers re-initiation and longer training. It provides extensive study on the GLUE benchmark showing how important these tricks are for small tasks (such as RTE/MRPC) which have less 1K training samples. The paper is well written and provides an insightful analysis.  \n\nAlthough it provides several useful tips for practitioners, it lacks novelty: for example the adam bias correction is from the original adam paper (also pointed it out by [2]) and training longer helping the performance is also observed by [1]. Gradient clipping may also help stabilize the training and it will be great to have a discussion as well. At last, does these approaches help large tasks, such as MNLI/QQP? It will be great to have a few settings: experiments on small or large tasks.  \n\n[1] Nakkiran et al, Deep double descent: where bigger models and more data hurt.\n[2] Mosbach et al, On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines\n", "rating0": "6: Marginally above acceptance threshold", "confidence0": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "review1": "The paper focuses on instability issues in BERT finetuning on small datasets. They list three factors which leads to instability, and provide simple fixes for each:\n1. Lack of bias correction term in BertAdam -- Fix was to use standard Adam \n2. Using all pretrained layers for finetuning -- Reinitializing the last few layers before finetuning. \n3. Training for a predefined number of epochs -- Train for a large number of epochs.\n\nThe fixes proposed reduces the variance in the results, and in most cases also improves performance. They also show that several proposed solutions to fix training instability lose their impact when the aforementioned fixes are incorporated. \n\nOverall, I like the paper; the observation about reinitializing top layers of BERT was interesting and counter intuitive to me; and I think this will be the most important contribution of the paper. \n\nAlthough not directly related to BERT, this paper (https://arxiv.org/pdf/1804.00247.pdf) also suggests training for longer epochs. This paper should be cited here. The tasks considered in the original BERT paper had large datasets, so I think the 2-3 epoch suggestion was tuned to those. \n\nThe result about BertAdam being unstable in low data settings, was a nice contribution. I feel this algorithm was also suggested considering the large datasets considered in the BERT paper.     \n\n ", "rating1": "6: Marginally above acceptance threshold", "confidence1": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review2": "### Summary\nThis paper investigates fine-tuning BERT for few-sample datasets. Notably, the authors find debiasing omission in BERT-adam. They find original debiased adam is better than BERT-adam. Besides, they also find re-initializing top layers can speed up learning and achieve better performance. These two findings are interesting. Another finding fine-tuning BERT for Longer is incremental to some extend.\n\n### Strengths\n * The two findings mentioned above are notable. \n\n* The authors conduct extensive experiments to support their claims.\n\n### Weaknesses and Questions\n* Table 1 shows the results of re-init but does not show re-init how many top layers for each task.\n* I suggest the authors can investigate debiased adam and re-init on the datasets with enough samples, like MNLI or QNLI. If they can achieve slight improvement or at least do not degrade the performance, we can just conveniently use the same fine-tuning method for most datasets.\n* Lack of explaining the meaning of Int. Task.\n", "rating2": "6: Marginally above acceptance threshold", "confidence2": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review3": "Large language models (LM) architectures, such as BERT, XLNet, etc., are not generally trained from scratch, but rather used as pretrained models. Among all, BERT is one of the most widely used ones, and its use on downstream tasks mainly consists on a stage of fine-tuning, where the new layers added are trained, and the rest of parameters of the network are left unfrozen, and hence are adjusted slightly to better fit the new task. However, this step of fine-tuning BERT is known to be quite unstable, and depends on a large set of factors, especially the initialization. Since the final performance on these downstream tasks can vary notably, different approaches has been proposed to circumvent this, but still the most common solution consists simply on choosing the best performing model, from a few random initialisation, using the validation set. \n\n##### Summary\nIn the current paper, the authors aim at tackling the aforementioned problem in a more grounded way. They investigate deeper the possible causes for these instabilities, and propose methods to counteract these pitfalls. Hence, they propose three simple, yet effective, approaches to stabilise the training and ensure better performances: a modified optimiser, the use of randomly initialised top layers, and more training steps. They provide a large collection of results, compare all these solutions to previous works, and discuss differences and similarities. Thanks to the analyses carried out, the current paper results in an exhaustive study on how \u201csafely\u201d fine-tune BERT, and the different factors that are to be taken into account when making use of these models.\n\n##### Strong and weak points\nI would like to start with the weakest point of the paper: it actually does not present anything clearly novel, nor innovative or groundbreaking. All the solutions proposed are inspired by previous approaches, or are just slight modifications of existing methods. But, this does not mean the paper is not valuable, as I do believe it is. The instability while fine-tuning large LMs on downstream tasks is a well known problem, but yet it has not been tackle exhaustively, and I do believe there does not exist clear guidelines and/or modifications that enable easily circumventing a critical weakness of these models. But I consider this paper succeeds at precisely this important task, thanks to the extended and exhaustive study it presents, and how it proposes three simple modifications that seem to solve this pitfall on most scenarios. \n\nBesides, the paper is quite well written, and presents in a clear manner the problems with the models, some intuition about the cause of those issues, and then, the solutions to overcome them. All the solutions are sufficiently justified, and are intuitive and simple. The latter, instead of being a weak point, for this precise problem it is more an advantage, as will allow an effortless adoption. Its improved performance is ensured thanks to the large set of benchmarks, on various datasets, the authors have compiled on the current manuscript. This is indeed another strong point, as all the solutions proposed are also tested under different conditions, with more or less training steps, and different numbers of top layers randomly initialised. \n\n##### Decision, and key reasons\nI believe the paper is ready to be accepted. Overall, it is an interesting and useful paper that will help many NLP researchers, and end-users of BERT, fine-tune better models, obtain improved performances, and therefore, start from a better baseline for their endeavours. And all this, with just some simple and intuitive modifications and guidelines. All the proposed methods and suggestions are not drawn from a few bunch of tests, but rather from a large collection of simulations, for different and varied datasets, with disparate starting conditions, and run over a fair amount of random initialisations. Therefore, I believe the authors have taken their time, and simulation time, to ensure that the presented results are robust and consistent, which is something to remark also.\n\n##### Questions and additional evidence\nAlthough I believe the paper is nicely written, and compiles all the required results and tests, I would appreciate if the authors could comment further on the following points:\n* I do believe there is a reason for not performing bias-correction on BERTAdam, and therefore, introducing it back might be affecting BERT training and fine-tuning in some specific, I guess negative, way. Could the authors comment on this? Or their understanding on why the correction was removed for BERTAdam.\n* In Figure 4, you suggest that with 5 to 10 random trials, the bias correction will achieve good results. However, observing the plots for all the datasets, we realise that indeed that number of random trials may benefit more the non-corrected version, as in most of the datasets the performance is either higher, or at least comparable. And although the variance is larger, we might still ensure at least a similar result . Could you comment on this? Would not be the corrected version a better option when no random initialisations are envisaged?\n* For the re-init, when just training for 3 epochs, it surprises me that indeed we could train the last 6 layers with just this reduced amount of data and training steps. And more surprisingly, according to Figures 14-16, is that the weights for these last 6 layers are the first to stabilise, even though they started from scratch, and they are supposed to be critical for the downstream tasks. Could you comment on this? I guess my understanding is wrong, and I would appreciate therefore some further insights. \n* Also, on the Re-init scheme, you mention that the number of layers to re-initialize depends on the task. Could you in any case offer here a general rule of thumb? \n\n##### Extra feedback\nFinally, I would like to conclude listing some small typos and errors I could spot in the manuscript:\n* Page 7, after Results, the reference to the Table is wrong.\n* Page 8, table 2: I believe the result for the RTE - Int. Task is mistype. I guess it should be something around 71.8.\n* Page 14, section E, Effect of Re-init\u2026 : the reference to the figure.\n* The caption for all figures 14 to 17 is wrong, as it should read fine-tuning.\n\nThese are the ones I could find, but it is not an exhaustive list. In any case, I would like to highlight the quality of the present manuscript, in terms of clearness and writing. ", "rating3": "7: Good paper, accept", "confidence3": "3: The reviewer is fairly confident that the evaluation is correct"}}, {"forum": "tUNXLHsIx3r", "title": "Paper Review", "reviews": {"review0": "The authors build a recommendation system that takes trust into consideration. The authors consider 3 facets of trust: local\ntrust, category-based global trust, and global review-feedback based trust.  The trusts are modeled using heuristic-based ratings. These trust ratings, together with user-user-based similarity are fed into a feed-forward neural network to generate the final predictions for the recommendation system. The performance is estimated on the Epinion dataset.\n\n[Pros]\n1.Considering side-information like user trust(social ties/social connections/social impact) is a meaningful application.\n\n[Cons]\n1. My major concern is that the proposed method lacks technical contribution.  As the authors pointed out, a multi-faceted trust-based recommendation system is not a new idea and has already been studied in numerous previous works. The 3 facets of trust proposed by the authors and the modeling of them using heuristic-based ratings are just common-sense.\n\n2.Moreover, the authors use a deep neural network for the recommendation task. However, the input of the neural network only comes with a dimension of 6, which is surprisingly small. The power of a deep neural network is to learn complicated patterns through big data. Why not directly model the recommendation and trust end-to-end using a single deep neural network model(In the paper, the trust scores for the 3 facest are all generated by heuristic instead of directly learned/modeled). I think this is a fundamental problem with the current algorithm design.\n\n3.Last but not least, the setup, modeling, and formulation of the problem seem tailored to the Epinion dataset, which seems to lose generalizability and further constrains the contribution of this paper.\n\nBased on the reasoning above, I recommend rejecting this paper.", "rating0": "4: Ok but not good enough - rejection", "confidence0": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "review1": "This paper proposed to take multiple facets of trust into account while looking at how suitable a product might be for a particular user, known as Social/Trust Recommender systems, which is usually to alleviate the cold start/sparse issue in recommender systems. The proposed MFTBR method provides a modular architecture for trust-based recommendation systems that allows the integration or removal of any number of trust facets, leading to different facet combinations.\n\n\nThe presentation of the paper can be significantly improved. The technical contribution of this paper is mediocre.  The proposed MFTBR piles up several similar modules but lacks adequate justification on why they are chosen for trust facets and why they can benefit the social/trust recommender systems. The idea of trust facets, including user-user similarity, local trust, category-wise global trust, and global review feedback, has already been incorporated in some relevant methodologies in different ways (e.g., TrustMF, TrustSVD, etc.). The combination is not particularly novel, and it is not clear if other advanced strategies can be adopted. It would be better if the authors could explicitly state the motivation of the design regarding the proposed trust facets in a clearer way. The neural network architecture illustrated in Figure 3 is not clearly presented in the text. Especially why the dimension in the input space is 6?\n\nIn experiments, the authors only use one small dataset to evaluate their effective performance.\nIt would be more convincing if other representative datasets, like Douban, Flixster, ciao, can be used to evaluate the performance. Furthermore, it would be better if more state-of-the-art baselines can be considered in experiments to validate the superiority of the proposed methods. For example, recently proposed GNN/DNN-based social/trust recommender systems can be considered as baselines. The experiments are not detailed enough. The experiment setting is not clearly stated. It is also not clear how their proposed method can address the cold-start and sparsity issue. It would be better if they could analyze the impact of different components (user-user similarity, local trust, category-wise global trust, and global review feedback).\n", "rating1": "4: Ok but not good enough - rejection", "confidence1": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review2": "\n1.\tQuality\nThe structure of the model and experiments are relatively complete, but the paper is poorly written. Also, the quality of English needs improving.\n2.\tClarity\nThe writing and expression of the paper need a lot of modification which is not very clear and fluent. Some problems are:\n\uf06c\tToo many references are cited in the abstract. There are 4 lines of \"recent research\" in the abstract that is not necessary.\n\uf06c\tOne section is need to define the problem.\n\uf06c\tDataset is introduced in the second section which should be put in experiment.\n\uf06c\tIn the part of research status, you just list some references. Paragraph that combine the research status with the research of this paper is need.\n\uf06c\tStrange statement: For example, \u201cIf user u likes item I and has a friend user k, SocialFD not only branch u and I closer but also branch k and I, propagating trust.\u201d in the last line of section 3.1.\n\uf06c\tGrammatical mistakes such as \u201cThe Facets Considered Were: Social relation between U and V-derived from Jaccard's Similarity and explicit Trust Statements, feedback on V's Profile, and the global feedback on V's Review of Item I.\u201d in Section 3.3.\n\uf06c\tIn 4.1, the overview is given in a nutshell\n\uf06c\tIn 4.2, the architecture should not be described just in a picture.\n\uf06c\tSome of the necessary references are missing.\n\uf06c\tLack of experiment environment settings, parameter settings, etc.\n\uf06c\tOnly MAE, RMSE, RC is compared with baseline, and it is recommended that at least the runtime be evaluated.\n3.\tOriginality\nThe author combined 4 modules together: User-User similarity-based predictor, Local trust-based predictor, Category global trust-based predictor wise, Global review feedback-based predictor. The trust facet weights are computed through the neural network for optimal results, which is a measure of dynamicity is introduced which is lacking in previous models. The extensible and dynamic nature of MFTBR ensure that it remains relevant and accurate in various scenarios.\n4.\tSignificance\nPropose a modular architecture for trust-based recommendation systems that allows the integration or removal of any number of trust facets. The proposed method outperforms the baselines in MAE, RMSE and RC.\n5.\tPros and cons\n\uf06c\tPros: Consider global and category. Generate viable predictions even when data is sparse.\n\uf06c\tCons: \nThe statement or organization of the model is not clear enough in this article. More detailed explanation is suggested. There are so many errors in the manuscript.\nThere is the lack of enough experimentation to demonstrate the validity and applicability of the proposed method. The author needs to do more experiments with more angles and show them in this paper. For example, the paper did not record the running time, which made the experiment not convincing enough.\nThe method of this paper is not innovative enough. In fact, most of the work is done by combining other people\u2019s methods. The authors need to highlight their innovative contributions.\n", "rating2": "4: Ok but not good enough - rejection", "confidence2": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review3": "In this paper, the authors focus on modeling multi-faceted trusts in collaborative filtering for rating prediction, which is a well-studied problem in recommender systems since the very beginning of early 1990s. In particular, the main idea is to use a neural network to learn the weight on the facets for better incorporation in final prediction. The authors then conduct empirical studies on one single dataset.\n\nSome comments:\n\n1 The organization of the paper can be improved significantly. For example, Section 2, Section 5 and Section 6 can be merged into one single section. \n\n2 The technical contribution is limited, e.g., the proposed solution is a two-staged heuristic solution based on neighborhood-based collaborative filtering and neural network.\n\n3 The results on one single dataset are not convincing to draw the conclusions.\n\n", "rating3": "3: Clear rejection", "confidence3": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}, {"forum": "igkmo23BgzB", "title": "Not sure if this paper is technically sound", "reviews": {"review0": "\nThis paper introduced a log-barrier based regularization method to reduce the dynamic range of data types (activation, weight, error, gradient, and input) in neural networks. The authors claim that such regularization is important to avoid overflow or swamping in the accumulation of matrix multiplication. \n\nHowever, the reviewer is afraid to think that there are serious technical issues with this claim. Here are a few details regarding this concern.\n\n- The main claim of this paper seems to be that by reducing the dynamic range of the neural network data, they can avoid overflow. However, it does not seem to be (always) true; what matters more would be the resolution of information you need (or want) to keep, and the number of data elements you accumulate. E.g., assume that we use 8-bit -- even if the magnitude of data is constrained to be around 2^-3, if the resolution of info is 2^-8, there will be an overflow after 8 times of accumulation. Also, note that the magnitude of activation or weight of a layer is often not important due to scale-invariance of the normalization techniques (like batchnorm). Thus it is not clear if constraining the magnitude of data is always beneficial.  \n\n- The authors claim that previous reduced-precision training techniques (like [Sun et al., NeurIPS19]) have dynamic quantization range across the layers. This is NOT true, since the reduced-precision floating point format has the fixed dynamic range; e.g., [Sun et al., NeurIPS] employs (1-5-2) format for representing back-prop error, implying the fixed dynamic range of 2^(-15) -- 2^(16). In fact, one very compelling benefit of using the reduced-precision floating-point over the fixed-point format is that the floating-point format does not need scaling of the magnitude across the layers. \n\n- It is not clear if the authors' understanding of \"overflow\" or \"swamping\" is the same as what is reported in prior work (e.g., [Sun et al., NeurIPS19]).  First of all, overflow and swamping issues are different - the former issue is for the fixed-point accumulation where the accumulated value wraps around to the small value when it exceeds the largest value representable by the given accumulation precision. Whereas, the latter is for the accumulation of floating-point numbers, where the small magnitude value is ignored (or truncated) when it is added to the large magnitude sum. Thus, it is very confusing when the authors use these two different terms interchangeably in the text (e.g., the 3rd paragraph of Intro). \n\n- There are many confusing points in the analysis of the overflow in Sec 3. First of all, the authors seem to claim that Prob(z > 2) is the overflow condition... but why? I couldn't find the notion of bit-width in this analysis... but then how can we check the overflow?\n\n- The authors mention that they employ STE, which does not make much sense in case of the quantized training; what does it mean to take STE for the quantization of back-prop error?\n\n- It is not clear how the proposed method is evaluated. The computational graph in Fig 2 does not seem to include the accumulation quantization. Also, more detailed information should be provided about the following; how the MU8 format is implemented to use 8-bit hardware? how the proposed method is implemented in the deep learning framework?\n\n\nTo sum up, this paper seems to have several technical flaws, which should be carefully addressed. \n", "rating0": "3: Clear rejection", "confidence0": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review1": "The paper is addressing an important and challenging problem of end-to-end training of deep nets in fixed-point, in this case, with 8-bit precision. A good solution to this problem can have a major impact on the deployability of deep nets on embedded hardware. The basic idea is to introduce an additional term (the log-barrier constraint) in the loss function to constrain the allowable range over which model parameters are allowed to take values. The authors use of mu-encoding to assign non-uniform quantization levels to minimize the quantization error. The main results are in Table 2 showing that the method eliminates overflow in practice and allows quantized networks to approach the accuracy of full-precision networks on the MNIST, CIFAR-10 and ImageNet.\n\nA few comments/questions:\n\n1) How was the quantization range ([-u,+u]) chosen? There will be a trade-off between the precision (8-b vs. 6-b) and the quantization range u. Is there any notion of optimality of the chosen value of u for 8-b quantization.\n2) How important is mu-law coding? How much accuracy is lost when uniform coding within the quantization range is applied? Please note, doing arithmetic on non-uniformly quantized variables is harder than with uniform coding, which has a bearing on hardware realizations. It will be good to quantify the accuracy loss in the absence of mu-law quantization.\n3) The results in Table 2 are sparse. It further indicates that the accuracy of MobileNet using the proposed method is quite significantly lower (66% vs. 72% for Sun) compared to previous works. MobileNet is designed for embedded IoT type hardware. 8-b or even 9-b quantized MobileNet results would put this work firmly in the domain of end-to-end fixed-point training of lightweight networks and hence make it more useful. These results will strengthen Table 2 significantly.\n4) Please comment on the choice of hyperparameters and initialization. Is this method robust to these choices?\n5) How does 8-b fixed-point compare with say MiniFloat-8 or equivalent? Such low-precision floating-point realizations are an attractive choice and a competitor to fixed-point.  \n6)The paper is missing comparisons with a couple of highly relevant papers on fixed-point training listed below:\n\n[1] Zhang et al., Fixed-point Back Propagation Training, CVPR 2020.\n[2] Sakr et al., Per Tensor Fixed-Point Quantization of the Back Propagation Algorithm, ICLR 2019.\n\nOverall a very nice paper but needs more work (indicated above) to strengthen the results.\n", "rating1": "6: Marginally above acceptance threshold", "confidence1": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "review2": "The paper presents a set of well crafted regularization techniques that improve the accuracy of QAT.\n\nThe overview of related works, particularly QAT is very well done. We clearly see what the present limitations are. I would like to point out that there have been works that have analytically determined suitable fixed quantization ranges in the context of backprop. The authors claim otherwise, I would suggest including works such as [1] in section 2.2, and revisit that claim.\n\nThe proposed method method is very nice, Lagrangian optimization techniques are used to ensure weights do not fall into the tails of the distribution. This reduces the probability of overflows. \n\nI wish to point out that the term 'swamping' does not describe an overflow phenomenon per se, it is rather an underflow that occurs when adding two very different floating-point numbers. Avoiding tail samples is definitely useful to reduce swamping, but some of the wording in section 3.3 and in the introduction probably ought to be revisited. It is not a very serious issue, but the clarity in that regard could be improved. Otherwise, the analysis in Section 3 is excellent.\n\nOne question about eq. (9): the tail distribution is defined as P(z>2) (subscripts aside for convenience). Shouldn't it be P(|z|>2)? Won't large negative numbers be problematic as well? In the grand scheme of things this won't affect the conclusions, but I think this correction (or at least a comment) should be made for extra rigor. \n\nThe experimental results are good. I wonder if the authors could spare a few sentence to discuss how they simulated accumulation quantization. This issue is not as straightforward as representation quantization that would apply to activations/weights/gradients. The cited papers that have looked at swamping previously (Wang & Sakr) do mention something about modifying the deep learning framework at the GEMM level. Is the same technique used?\n\nOne question about a statement made in the conclusion: Throughout the paper, a clipping level of 2 has been chose, but in the final discussion, it is stated that the weights are contrained to the range [-1.5,2.5]. Why the disparity? Is this an error or something expected? If so, can it be explained?\n\nMinor issue:\nIt is stated in section 3 that \\phi_t approaches the indicator function when t approaches infinity. How so? I get that z<0 -> \\phi_\\ifty = 0 but z>0 is not well defined. Maybe the definition of \\phi for z>0 can be explicitly stated.\n\n[1] Sakr, C., & Shanbhag, N. Per-tensor fixed-point quantization of the back-propagation algorithm. In 7th International Conference on Learning Representations, ICLR 2019.\n\n\n=======================================================================================================\nComments Post Rebuttal:\nI still find the technical contribution of this paper to be a good one. However, As stated in my original review, there were some clarity issues with the manuscript. While, I personally was able to follow along, the other reviewers are correct in their claim that the writing can be quite confusing (Reviewer 4 makes a strong case). To that end, I have decided to decrease my score to a 5 because I can no longer say the manuscript is ready for publication. Nonetheless, I urge the authors not to be disappointed. The presented work has many merits, and I am sure that with a thorough \"clean up\" of the text, this paper can be a great one!", "rating2": "5: Marginally below acceptance threshold", "confidence2": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "review3": "The authors introduce a log-barrier extension loss term enforcing soft constraints on the range of values to enable fully end-to-end quantization-aware training. \n\nStrengths of the paper: \n\n- The paper addresses an important topic, because there are increasing concerns in performing fully end-to-end low precision training to deploy on low-precision hardware. \n- The method has a practical goal and could be interesting for practitioners \n\nWeaknesses of the paper: \n\n- Lack of positioning with respect to the SOTA quantization-aware training(QAT) and post-training quantization(PTQ) schemes, there are plenty of missing related literatures on both quantization schemes. Some statements in the background and related work could be wrong. For example, QAT also focuses on efficient inference as well as PTQ. The levels of practical applicability of a variety of quantization solutions have been introduced in DFQ(Nagel etal., 2019). Survey on the related work is not sufficient. \n- Having a benchmark would be interesting if it will include some SOTA methods and evaluates with them. The comparison targets are mostly out-of-date. It is lack of convincing evaluation results to support the proposed scheme.\n- Organizing the whole contents is ok but not good enough for the readers to easily follow and understand. \n\nDetailed comments: \n\n(1) The terminology on swamping might not be familiar with the ML community. Explaining the criticality of swamping problem is not good enough in the intro. You should provide how critical the problem is on the low-precision hardware with the other SOTA quantization schemes. For example, the probability of occuring swamping without applying the proposed scheme, etc.\n\n(2) Evaluation results provided in the paper are just for comparing accuracy. Accuracy loss is intrinsic in fully end-to-end low-precision training. The benefits of employing the proposed scheme would be beyond accuracy, say memory or energy-saving constraints for on-device training. Experimental evaluations to support the necessity and merits of the proposed scheme should be provided. \n\n(3) The quantization range is fixed in the proposed scheme. Is it a merit that the proposed scheme does not need to adjust the range and precision either per-layer or per-channel during training as in other SOTA methods?\n\n(4) Writing on the constrained optimization formulation is a bit verbose and not properly formulated. \n\n(5) Inducing the tail bound of distribution to demonstrate that the probability of swamping can be controlled, several assumptions and approximations have been applied for the worst-case upper bound. Are the assumptions reasonable to work in practice? For example, assuming that the weight distribution is Gaussian is too strong to be practical. \n\n(6) The paper has a conceptual overlap with other quantization approaches and some of the proposed scheme is not entirely novel resulting in a weak contribution. \n\n(7) In Table 2, the MobilNet has more severe degradation of accuracy than the ResNet on the low-precision(8-bit) setting. Could you explain why this happens?\n\nMinors: \n- Several typos: There was been in p.2, to soft threshold the range of in p.3, theta-i in eq.(3), some more in p.7", "rating3": "3: Clear rejection", "confidence3": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}, {"forum": "Ggx8fbKZ1-D", "title": "Ambiguous analysis and results from a novel method", "reviews": {"review0": "Quality\n\n+ The authors have published code for replicability.\n+ The baseline evaluations are of reasonable quality.\n+ The proofs and theorems around convergence and complexity are high quality.\n- The results on their metrics are poor and ambiguous. They only evaluate on validation accuracy / cross entropy, which do not improve substantially.\n- Combination ratio results look inconsistent across datasets & models. \n- The analysis is limited and there are few generalizable insights to be gleaned from the paper.\n- There\u2019s no transfer analysis or generality analysis, implying that each task will have to have its hyper-parameters tuned independently.\n- It\u2019s not demonstrated that overparameterization is a problem. Networks are already overparameterized without issue.\n\nClarity\nTheir combination ratios plot is unclear. What should be taken away from these changes in Gamma? The analysis is unclear. Convergence analysis is unclear. There are plenty of typos. \n\n\nOriginality\n\nThe regularization methodology is somewhat novel, to my knowledge. Hypergradients are known and this is an extension to more parameters which will interact with one another. Multiple levels of tree based interacting hyperparameters is novel, to my knowledge.\n\nSignificance\n\nThis result isn\u2019t a substantial improvement over existing methods. There aren\u2019t clear insights to be gleaned that will generalize to other work on hypergradients (other than that it\u2019s possible to try to regularize with a hierarchical parameter scheme).\nThis may be evidence that trying to tune every parameter simultaneously is overkill / too challenging.\n\n\n\nPros\n\nThe authors present a novel method for learning hyperparameters of a model at multiple levels which typically are not manually tuned. The potential upside of a working method here is high, as the learning rate dramatically impacts model performance. They publish code for reproducibility. The authors propose and implement a novel regularization method for their new hyperparameters as well. There are proofs backing convergence claims made by the authors. The baselines have been tuned and the presentation is honest.\n\nCons\n\nThe paper\u2019s results are ambiguous. The paper isn\u2019t carefully written - it\u2019s unclear what \u2018levels of adaptations\u2019 refer to until after the introduction. It\u2019s not clear what conclusions should be drawn from the combination ratios graph as gamma changes (Figure 1). The paper\u2019s analysis value is limited, it\u2019s unclear what solid & general insights about hypergradients can be taken away from it.\n\n\n\nOther feedback:\n\nTypo in the abstract: combination -> combinations\nTypo in the first line of introduction: is gradient -> is the gradient\nTypo in the second sentence of the second paragraph of the introduction: function -> functions\nTypo in the third paragraph of the introduction: horizon -> horizons\nTypo in first paragraph of Experiments: lowercase p in \u2018The Proposed -> The proposed\u2019\nLikely better not to shorten \u2018Feed Forward Neural Network\u2019 to FFNN in the section heading (though this is fine in the body of the text).\n", "rating0": "5: Marginally below acceptance threshold", "confidence0": "3: The reviewer is fairly confident that the evaluation is correct", "review1": "Update: I really appreciate the response from the authors. Some of my original concerns have been addressed, and additional experiments help to show the benefits of CAM-HD, so I have increased my score to 5. But, after reading other reviews and responses, I still believe that this work needs to be compared to advanced learning rate adaptation methods. Most reviewers have pointed out the presentation and insufficient experiments, so it's better to submit the improved version to one of upcoming conferences.\n\n**Summary**\nThis work proposes an optimizer that adaptively determines a learning rates from different levels (global, layer-wise, parameter-wise) based on the hypergradient framework.  The proposed optimizer introduces many additional hyperparameters, and empirical evidence is not strong compared to baselines.\n\n**Detailed comments**\n\nThe proposed method adaptively adjusts learning rates at different levels (parameter-wise, layer-wise, and global). This needs to be compared to previous learning rate schedulers, but I found that only a basic scheduler has been compared. How about the performance of CAM-HD compared to previous approaches?\n* Using Statistics to Automate Stochastic Optimization, NeurIPS\u201919. \n* Statistical Adaptive Stochastic Gradient Methods, Arxiv\u201920. (This work is an extension of the neurips paper above).\n* Large Batch Training of Convolutional Networks (Arxiv\u201917). This work also proposes a ayer-wise Adaptive Rate Scaling.\n* SGDR: Stochastic Gradient Descent with Warm Restarts, ICLR\u201917.\n\nThis seems to be very sensitive to newly introduced hyperparameters (beta and gammas). This method requires extensive grid search to find a suitable beta, because the optimal beta is different across tasks (as in Table 1). \n\nThe proposed optimizer has been validated only on small-scale datasets. It\u2019s difficult to predict how to behave in training on a large-scale dataset (such as ImageNet). In addition, I\u2019m not quite sure that this approach works well with strong data augmentation techniques (for instance, auto augmentation). Also, I guess that the proposed scheduler will not be working well on large-batch settings (for instance, batch size >= 1K). \n\n\u201cFor the learning tasks with recommended learning rate schedules, we will apply these schedules as well.\u201d -> In experiments, just a simple step decay lr rule has been applied. What about using cosine annealing lr schedulers? It could improve the baseline consistently.", "rating1": "5: Marginally below acceptance threshold", "confidence1": "3: The reviewer is fairly confident that the evaluation is correct", "review2": "Setting appropriate learning rate for network optimization is an important task in deep learning applications. This paper investigates the setting of learning rates for network parameters in different levels, e.g., individual parameter, each layer and global levels. By setting the constraints on the learning rates at multiple scales, the paper derived a hierarchical learning rate setting approach, which is the combination of adaptive learning rates at different levels. \n\nOverall, this proposed learning rate setting method seems to be interesting, however, I have some concerns on the setting of the hyper-parameters of the proposed method. \n\n1. The paper proposed to set the constraints at different levels by Eqn. (7). I have concern on how to automatically set the hyper-parameters of combination weights.  The details on setting / learning these hyper-parameters should be clarified in a more clear way. \n\n2. What are the possible reasons that the proposed hierarchical learning rate can improve the baseline optimizer Adam and SGD?\n\n3. The proposed learning rates setting method can be combined with any gradient-based network optimizer. More combinations and corresponding results and comparisons should be given to show how much the proposed technique can improve the different baseline gradient-based optimization methods.  \n\n4. The comparisons with more network optimizers should be given in the experiments. \n\n\n----\nPost rebuttal comments\uff1a\nThanks for the responses from the authors. These responses partially solved my questions. I think that the initialization of hyper-parameters of combination weights seem to be heuristic, and it is unclear on the effects/robustness of its initialization on the optimization performance. My questions on more comparisons and more combinations with other optimizers are not well answered. \n\nI also read the other reviews and responses, I agree with other reviewers on the concerns of experiments, justifications, robustness, etc. Considering that it needs some additional works to solve all these concerns, I suggest the authors to improve the paper and submit it to one following conference. ", "rating2": "5: Marginally below acceptance threshold", "confidence2": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review3": "### Summary of the paper\n\nThe paper investigates the setting of hyper-gradient descent in the context of adapting learning rates at different levels in a neural network (e.g. per layer). The paper derives an equivalence between regularization of such learning rates and a weighted combination of non-regularized adapted learning rates. Experiments on small datasets and small network architectures are performed and show an improvement over baselines including Adam and SGD.\n\n### Main review\n\nAfter the introduction (which is well-written) I found the paper relatively hard to follow. Sections 2.2 and 2.3 are long and introduce a large variety of options that *could* be used without giving the reader a clear perspective of where this discussion is going. In the experiments, a lot of different combinations of small networks and datasets are used, and while the learning curves look promising, it is hard to assess how robustly the improvements may transfer to other settings, because e.g. the parameters $\\gamma$ are initialized differently in the experiments. The experiment of Fig.1 shows that the variation in the measurements can be very high with respect to $\\gamma$, such that the effect of the algorithm as such is a bit hard to assess. (And e.g. Fig.7 seems to indicate that the $\\gamma$s sometimes do not move from their initial values.) This makes me wonder how much of an improvement it is to go from optimizing hyper-parameters of e.g. SGDN or Adam to optimizing \"hyper-hyper-parameters\". \n\nWhen reading the paper I also wondered if any relation between the behavior of the adapted learning rates and other quantities in training (e.g. momentum per parameter) can be established. Does the adaptation counteract or increase momentum effects? Or does this depend on other quantities? Any such connection could provide insight and/or help with intuitions about the adaptation behavior. I did not see any attempt into such a direction in the paper.\n\nThe relation between regularization and weighting of unregularized values (Theorem 1) seems interesting.\n\nAs the experiments go as far as ResNet-34 on CIFAR10, it would be interesting if a comparison to existing results from the literature could be made. E.g. the accuracies in Fig.4 seem to go into the area of 92-93%, which seems appropriate for this combination, but it is hard to tell from the graph alone. (It would be even more impressive if a clear statement could be made that shows that the final accuracy is better than for the optimizers used in a work that is (close to) state of the art (under given constraint like e.g. architecture=ResNet-18).\n\nA (brief) discussion of the additional computational cost (in terms of memory and operation count) in the main body text would be useful. \n\n* Pros: The paper takes hyper-gradient to many new multi-levels.\n* Cons: The paper is somewhat hard to follow, and it is not immediately clear how transferable the experimental results are to other settings.\n\n### Minor details and comments\n* The font in the figures is very small in some cases, e.g. Figure 5 is very hard to read even when zooming in considerably on the screen.\n* Typos/nits:\n  * p.3 \"leave nodes\" -> leaf\n  * p.4 \"use each of them to updated the learning rate\" -> update\n  * p.6 \"tunning\"\n  * p.8 \"in later stage\"; \"our methods achieves\" -> method\n  * in several places: CMA -> CAM\n  * Lowercasing in reference titles, e.g. \"Rmsprop\", \"smd gain\"", "rating3": "5: Marginally below acceptance threshold", "confidence3": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}, {"forum": "P42rXLGZQ07", "title": "Ok theory; Clarity can be improved; application impact need to considered ", "reviews": {"review0": "This paper proposes to use evolutionary algorithm to learn truncated deep latent variable model. The method get good performance in denoising  task. \n\nPros:\nQuality: The method seems correct \nOn denoising task, the performance of the proposed model is good. \nSignificance: Inference of Discrete VAE is an important question to address\n\nCons & questions:\nClarity: The paper is very hard to read due to many reasons: \na) It does not use commonly used notations, such as the paper use F to present ELBO. The paper seems to use theta to present the decoded mean and variance where people commonly use theta for the decoder weights etc. Under such unique notations which is not wrong, but the author did not explain them clearly either. Around equation (2), it just says that these are parameter to optimize (which made me think that these are the weights) and later on I found that weights are denoted by W.  and phi is the Bernoulli parameter for z (which was denoted as pi before) and theta is the decoded mean and variances. So it just makes reading very confusing. \nb) It is not self contained. For example equation (7) is just pointed to another paper which I checked another paper for mins and did not find equation (7)  and did not try more as the cited paper is very very long. So it would be good to clarify critical equations. \nI also wonder about Eq.(7)'s correctness as the right hand side looks like the model evidence instead of the lower bound of it. \nOther places, such as Eq.(3) it is formulated in this way in the Truncated VAE paper but not cited. \nGenerically, the reading of the paper is not easy and there are many simple things to do just to make it more clear. \n\nSignificance:\na) The author seems try to claim that people in the field never used non-amortized way to train a deep latent Gaussian model. This is very wrong, also the author did not discuss related work in this direction at all. e.g. using MCMC\nLearning Deep Latent Gaussian Models with Markov Chain Monte Carlo, http://proceedings.mlr.press/v70/hoffman17a/hoffman17a.pdf \n\nb)  VAE's contribution are two-fold, one is latent variable model and the second part if the amortized inference. The paper does not seems differ such existing separation. The paper in the end contributed a non-amortized inference method, but the discussion of the paper is very entangled.  \n\nc) As the author pointed out, the proposed method is not scalable  as it is per data point and thus it will have limited application impact. \n\nd) Experiments did not compare any highly relevant inference baselines. It does not compare any other inference methods in deep latent variable model. \n\ne) there is only one set of experiments in total and on an traditional denositing task which is very limited. More experimental settings are needed to show the usefulness of the method. \n", "rating0": "5: Marginally below acceptance threshold", "confidence0": "3: The reviewer is fairly confident that the evaluation is correct", "review1": "This paper proposes an evolutionary optimization framework for training vartional autoencoders (VAEs) with discrete latents. In contrast to the standard VAE paradigm, the proposed TVAE approach does not require an encoder for amortized inference given the input. The method instead relies on a pool of latent variable samples for each data point to activate the decoder network. The latent variable pools are maintained and iteratively updated to increase the average lower-bound of the marginal log-likelihood of the input data. Experimental results show that non-linear decoders optimized by the TVAE framework outperform their linear counterparts on a denoising task.  Further results demonstrate method's competitiveness on zero-shot denoising, where a TVAE decoder is only trained on the noisy input image to reconstruct a smoothed version of the input image.\n \nThe paper is well-written and easy to follow. The work proposes an interesting alternative for optimizing VAEs which does not require an encoder network for amortized inference. I however have a number of concerns that are as follows:\n\nThe method instead imposes the overhead of maintaining and evolving a collection of latent variables for every data point, which can be both sample inefficient and memory-heavy for sizable problems. Then when or why would one trade amortized inference for the proposed approach?\n\nThe paper falls short of comparing the proposed optimization procedure with other alternatives for training standard or discrete VAEs. \n\nFrom reading the paper it is not clear how the size of the pool may need to be varied as the nature of the task or the number of latent changes.\n\nThe authors use a greedy approach to update the pool of latent samples. Does it not cause the optimization procedure to get stuck in local modes? Could one instead use MCMC type sampling approaches to enable mode jumps? \n\nWhy only denoising experiments? Can the authors use their approach for data synthesis tasks where the latents can be shown to control different attributes of the data generative process?", "rating1": "6: Marginally above acceptance threshold", "confidence1": "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "review2": "This paper proposes a new approach to train VAEs with binary latents, using an evolutionary algorithm to optimise a discrete set of variational parameters rather than the usual amortised variational model trained with gradient-based methods. The authors consider the setting of training a VAE with discrete, Bernoulli distributed latents and a continuous, Gaussian distributed output. They use a novel approach to form and train a posterior on the discrete latent space, parameterising the posterior somewhat implicitly via the sets defining a truncated posterior - in which the approximate posterior is proportional to the true posterior, but only within a subset of all points. Defining the subset of points amounts to parameterising the approximate posterior. Optimisation of these parameters amounts to a discrete search problem, which the authors tackle using an evolutionary algorithm.\n\nOverall, I find the paper very well written and straightforward to follow. Indeed, the style of writing tends to pique the interest of the reader without being colloquial. I think the motivation of the paper is well established, given the number of papers that have investigated the training of modern generative models with discrete latents. Importantly, I think the proposed approach is novel and seemingly effective. The use of the truncated posterior along with a relatively simple evolutionary optimisation procedure is quite different from the previously seen methods that generally attempt to maintain a differential proxy to the ELBO.\n\nThe proposed method does have limitations, which the authors acknowledge. Namely that the method will not scale to large datasets, since the posterior is not amortised. I think it must also be true that the evolutionary optimisation is not particularly suited to high-dimensional spaces, since it is generally accepted that search suffers in such settings. However, these limitations I do not think are overwhelming. The authors find a niche problem setting in which their method is practical - that of \u201czero-shot\u201d data denoising. In this setting they demonstrate that their method (with a small VAE) outperforms the state-of-the-art methods. The authors also provide a small set of straightforward and convincing toy experiments to show that their approach satisfies basic properties that we require in a learning system, such as appropriate scaling behaviour and recovery of artificial data generating parameters. Although I think the empirical demonstration may be enough, given the method is relatively novel, it is still limited. The main experimental result of the paper is denoising on a single image. Even performing the denoising on a small set of images would help the paper, since there will undoubtedly be variance in the performance across different images.\n\nOne other thing I would say about this paper is that identifying the approach as a VAE almost feels inappropriate. It is already true that the usual VAE is not an autoencoder at all, and so the name is a slight misnomer. But with the authors\u2019 approach here, the posterior is not even parameterised as a function of the data (although there is of course an implicit dependence established through training). Hence identifying the method as related to an autoencoder seems to be misleading (although understandable given that the method is clearly related to the VAE).", "rating2": "6: Marginally above acceptance threshold", "confidence2": "3: The reviewer is fairly confident that the evaluation is correct", "review3": "**summary** \nthe paper proposes a novel approach to training variational autoencoder models, based on non-parametric form of truncated approximate posterior. Posterior is truncated to have support on a small subset of latent space allowing for exact marginalization. The support of approximate posterior in latent space for each data point $x$ is learned via evolutionary algorithm, minimizing ELBO. Method is applied to denoising tasks for images.\n\n**pros**\nthe idea of using evolutionary algorithm for approximating truncated posterior is new to me, the fact that it works in practice is very interesting. \n\n**cons**\n* The idea is limited to small architectures due to the need to evaluate the decoder for every state in the truncated set as opposed to single evaluation in conventinal approaches. The authors for this reason choose to apply their approach to denoising applications, which I am not familiar with and can't evaluate the advantage of their method.\n* The fact that approximate posterior is not amortized over the dataset is also a potential issue: for each data point $q(z|x)$ will need to be updated to match evolution of the decoder of the entire epoch. It seems plausible that applying more evolutionary steps can take care of this but it would be useful to see experiments with varying number of  evolutionary steps.\n* authors didn't compare their approach to the recent paper \"Efficient Marginalization of Discrete and Structured Latent Variables via Sparsity\" https://arxiv.org/abs/2007.01919, it would be good to understand which one works better.\n\n\ncomments:\n* The approach is reminiscent of EM approach where samples from the posterior are approximated via Metropolis-Hastings updates. This can be made more efficient by adding temperature annealing. I would be interested to see how the proposed approach compares to this.\n* it would be clearer to make x-dependence in Eq (3) explicit rather then just through superscript (n)\n* it would be interesting to see how large is gap between the proposed ELBO and true LL in the trained models, to get a sense of good the approximation is.\n\n\n", "rating3": "6: Marginally above acceptance threshold", "confidence3": "3: The reviewer is fairly confident that the evaluation is correct"}}]